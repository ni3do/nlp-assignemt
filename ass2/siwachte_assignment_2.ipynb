{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xxu7h8e4Cm9u",
        "outputId": "af232672-a4f8-45d9-de94-8a6095a669bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchdata==0.4.1 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (0.4.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from torchdata==0.4.1) (1.26.13)\n",
            "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from torchdata==0.4.1) (2.28.1)\n",
            "Requirement already satisfied: portalocker>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from torchdata==0.4.1) (2.3.0)\n",
            "Requirement already satisfied: torch>1.11.0 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from torchdata==0.4.1) (1.12.1)\n",
            "Requirement already satisfied: typing_extensions in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from torch>1.11.0->torchdata==0.4.1) (4.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from requests->torchdata==0.4.1) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from requests->torchdata==0.4.1) (2022.9.24)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from requests->torchdata==0.4.1) (2.1.1)\n",
            "Requirement already satisfied: torch==1.12.1 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (1.12.1)\n",
            "Requirement already satisfied: typing_extensions in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from torch==1.12.1) (4.4.0)\n",
            "Requirement already satisfied: transformers in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (4.24.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages (from requests->transformers) (1.26.13)\n"
          ]
        }
      ],
      "source": [
        "# Install miscellaneous libraries.\n",
        "!pip install torchdata==0.4.1\n",
        "!pip install torch==1.12.1 \n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5fQf34J3cJY-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Import libraries used throughout - if you need other libraries, \n",
        "# you are free to import them.\n",
        "import functools\n",
        "import random\n",
        "from typing import Any\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchtext\n",
        "import torchtext.functional as F\n",
        "import torchtext.transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.datasets import UDPOS\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "F4OW-ryOCwbG"
      },
      "outputs": [],
      "source": [
        "# Constants and hyperparameters - you are free to change these for\n",
        "# the bonus question.\n",
        "SEED = 42\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 3\n",
        "LR = 2e-5\n",
        "TRANSFORMER = \"bert-base-uncased\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lYinc-SdCzdZ"
      },
      "outputs": [],
      "source": [
        "# Reproducibility.\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.use_deterministic_algorithms(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YrTZG0UHC1YU"
      },
      "outputs": [],
      "source": [
        "# Setting up dataloaders for training.\n",
        "tokenizer = BertTokenizer.from_pretrained(TRANSFORMER)\n",
        "init_token = tokenizer.cls_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "sep_token = tokenizer.sep_token\n",
        "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
        "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
        "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
        "sep_token_idx = tokenizer.convert_tokens_to_ids(sep_token)\n",
        "max_input_length = tokenizer.max_model_input_sizes[TRANSFORMER]\n",
        "\n",
        "train_datapipe = UDPOS(split=\"train\")\n",
        "valid_datapipe = UDPOS(split=\"valid\")\n",
        "pos_vocab = build_vocab_from_iterator(\n",
        "    [i[1] for i in list(train_datapipe)],\n",
        "    specials=[init_token, pad_token, sep_token],\n",
        ")\n",
        "\n",
        "\n",
        "def prepare_words(tokens, tokenizer, max_input_length, init_token, sep_token):\n",
        "    \"\"\"Preprocesses words such that they may be passed into BERT.\n",
        "\n",
        "    Parameters\n",
        "    ---\n",
        "    tokens : List\n",
        "        List of strings, each of which corresponds to one token in a sequence.\n",
        "    tokenizer : transformers.models.bert.tokenization_bert.BertTokenizer\n",
        "        Tokenizer to be used for transforming word strings into word indices\n",
        "        to be used with BERT.\n",
        "    max_input_length : int\n",
        "        Maximum input length of each sequence as expected by our version of BERT.\n",
        "    init_token : str\n",
        "        String representation of the beginning of sentence marker for our tokenizer.\n",
        "    sep_token : str\n",
        "        String representation of the end of sentence marker for our tokenizer.\n",
        "\n",
        "    Returns\n",
        "    ---\n",
        "    tokens : List\n",
        "        List of preprocessed tokens.\n",
        "    \"\"\"\n",
        "    # Append beginning of sentence and end of sentence markers\n",
        "    # lowercase each token and cut them to the maximum length\n",
        "    # (minus two to account for beginning and end of sentence).\n",
        "    tokens = (\n",
        "        [init_token]\n",
        "        + [i.lower() for i in tokens[: max_input_length - 2]]\n",
        "        + [sep_token]\n",
        "    )\n",
        "    # Convert word strings to indices.\n",
        "    tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def prepare_tags(tokens, max_input_length, init_token, sep_token):\n",
        "    \"\"\"Convert tag strings into indices for use with torch. For symmetry, we perform\n",
        "        identical preprocessing as on our words, even though we do not need beginning\n",
        "        of sentence and end of sentence markers for our tags.\n",
        "\n",
        "    Parameters\n",
        "    ---\n",
        "    tokens : List\n",
        "        List of strings, each of which corresponds to one token in a sequence.\n",
        "    max_input_length : int\n",
        "        Maximum input length of each sequence as expected by our version of BERT.\n",
        "    init_token : str\n",
        "        String representation of the beginning of sentence marker for our tokenizer.\n",
        "    sep_token : str\n",
        "        String representation of the end of sentence marker for our tokenizer.\n",
        "\n",
        "    Returns\n",
        "    ---\n",
        "    tokens : List\n",
        "        List of preprocessed tags.\n",
        "    \"\"\"\n",
        "    # Append beginning of sentence and end of sentence markers\n",
        "    # cut the tagging sequence to the maximum length (minus two to account for beginning and end of sentence).\n",
        "    tokens = [init_token] + tokens[: max_input_length - 2] + [sep_token]\n",
        "    # Convert tag strings to indices.\n",
        "    tokens = torchtext.transforms.VocabTransform(pos_vocab)(tokens)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "text_preprocessor = functools.partial(\n",
        "    prepare_words,\n",
        "    tokenizer=tokenizer,\n",
        "    max_input_length=max_input_length,\n",
        "    init_token=init_token,\n",
        "    sep_token=sep_token,\n",
        ")\n",
        "\n",
        "tag_preprocessor = functools.partial(\n",
        "    prepare_tags,\n",
        "    max_input_length=max_input_length,\n",
        "    init_token=init_token,\n",
        "    sep_token=sep_token,\n",
        ")\n",
        "\n",
        "\n",
        "def apply_transform(x):\n",
        "    return text_preprocessor(x[0]), tag_preprocessor(x[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ikF2VKvqC2DS"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_datapipe = (\n",
        "    train_datapipe.map(apply_transform)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .rows2columnar([\"words\", \"pos\"])\n",
        ")\n",
        "train_dataloader = DataLoader(train_datapipe, batch_size=None, shuffle=False)\n",
        "valid_datapipe = (\n",
        "    valid_datapipe.map(apply_transform)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .rows2columnar([\"words\", \"pos\"])\n",
        ")\n",
        "valid_dataloader = DataLoader(valid_datapipe, batch_size=None, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KM0766voeCyX"
      },
      "outputs": [],
      "source": [
        "from heapq import heappush, heappop, heapify\n",
        "import itertools\n",
        "\n",
        "\n",
        "class PriorityQueue:\n",
        "    def __init__(self):\n",
        "        self.pq = []  # list of entries arranged in a heap\n",
        "        self.entry_finder = {}  # mapping of tasks to entries\n",
        "        self.REMOVED = \"<removed-task>\"  # placeholder for a removed task\n",
        "        self.counter = itertools.count()\n",
        "        self.length = 0  # unique sequence count\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def push(self, entry):\n",
        "        \"Add a new task or update the priority of an existing task\"\n",
        "        priority = entry[0] * -1\n",
        "        task = entry[1]\n",
        "        self.length += 1\n",
        "        if task in self.entry_finder:\n",
        "            priority = min(self.remove(task), priority)\n",
        "        count = next(self.counter)\n",
        "        new_entry = [priority, count, task]\n",
        "        self.entry_finder[task] = new_entry\n",
        "        heappush(self.pq, new_entry)\n",
        "\n",
        "    def remove(self, task):\n",
        "        self.length -= 1\n",
        "        entry = self.entry_finder.pop(task)\n",
        "        entry[-1] = self.REMOVED\n",
        "        return entry[0]\n",
        "\n",
        "    def pop(self):\n",
        "        self.length -= 1\n",
        "        while self.pq:\n",
        "            priority, count, task = heappop(self.pq)\n",
        "            if task is not self.REMOVED:\n",
        "                del self.entry_finder[task]\n",
        "                return (-1 * priority, task)\n",
        "        raise KeyError(\"pop from an empty priority queue\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8AgCIq_AC7K5"
      },
      "outputs": [],
      "source": [
        "class TagLSTM(nn.Module):\n",
        "    \"\"\"Models an LSTM on top of a transformer to predict POS in a Neural CRF.\"\"\"\n",
        "\n",
        "    def __init__(self, nb_labels, emb_dim, hidden_dim=256):\n",
        "        \"\"\"Constructor.\n",
        "\n",
        "        Parameters\n",
        "        ---\n",
        "        nb_labels : int\n",
        "            Number of POS tags to be considered.\n",
        "\n",
        "        emb_dim : int\n",
        "            Input_size of the LSTM - effectively embedding dimension of our pretrained transformer.\n",
        "\n",
        "        hidden_dim : int\n",
        "            Hidden dimension of the LSTM.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm = nn.LSTM(\n",
        "            emb_dim, hidden_dim // 2, bidirectional=True, batch_first=True\n",
        "        )\n",
        "        self.tag = nn.Linear(hidden_dim, nb_labels)\n",
        "        self.hidden = None\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (\n",
        "            torch.randn(2, batch_size, self.hidden_dim // 2),\n",
        "            torch.randn(2, batch_size, self.hidden_dim // 2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.hidden = self.init_hidden(x.shape[0])\n",
        "        x, self.hidden = self.lstm(x, self.hidden)\n",
        "        x = self.tag(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class NeuralCRF(nn.Module):\n",
        "    \"\"\"Class modeling a neural CRF for POS tagging.\n",
        "    We model tag-tag dependencies with a weight for each transition\n",
        "    and word-tag influence through an LSTM on top of a pretrained transformer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        pad_idx_word,\n",
        "        pad_idx_pos,\n",
        "        bos_idx,\n",
        "        eos_idx,\n",
        "        bot_idx,\n",
        "        eot_idx,\n",
        "        t_cal,\n",
        "        transformer,\n",
        "        lstm_hidden_dim=64,\n",
        "        beta=0,\n",
        "    ):\n",
        "        \"\"\"Constructor.\n",
        "\n",
        "        Parameters\n",
        "        ---\n",
        "        pad_idx_word : int\n",
        "            Index corresponding to padding in the word sequences.\n",
        "        pad_idx_pos : int\n",
        "            Index corresponding to padding in the tag sequences.\n",
        "        bos_idx : int\n",
        "            Index corresponding to beginning of speech marker in the word sequences.\n",
        "        eos_idx : int\n",
        "            Index corresponding to end of speech marker in the word sequences.\n",
        "        bot_idx : int\n",
        "            Index corresponding to beginning of tag marker in the tag sequences.\n",
        "        eot_idx : int\n",
        "            Index corresponding to end of tag marker in the tag sequences.\n",
        "        t_cal : List[int]\n",
        "            List containing all indices corresponding to tags in the tag sequences.\n",
        "        transformer : BertModel\n",
        "            Pretrained transformer used to embed sentences before feeding them\n",
        "            into the LSTM.\n",
        "        lstm_hiden_dim : int\n",
        "            Hidden dimension of the LSTM used for POS tagging. Note that\n",
        "            since we are bidirectional, the effective hidden dimension\n",
        "            is half of this number.\n",
        "        beta : float\n",
        "            Regularization hyperparameter of the entropy regularizer.\n",
        "            Entropy regularization is only applied for \\beta > 0.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.pad_idx_word = pad_idx_word\n",
        "        self.pad_idx_pos = pad_idx_pos\n",
        "        self.bos_idx = bos_idx\n",
        "        self.eos_idx = eos_idx\n",
        "        self.bot_idx = bot_idx\n",
        "        self.eot_idx = eot_idx\n",
        "        self.t_cal = t_cal\n",
        "        self.transformer = transformer\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.beta = beta\n",
        "        self.transitions = nn.Parameter(torch.empty(len(t_cal), len(t_cal)))\n",
        "        self.emissions = TagLSTM(\n",
        "            len(t_cal),\n",
        "            transformer.config.to_dict()[\"hidden_size\"],\n",
        "            lstm_hidden_dim,\n",
        "        )\n",
        "        self.init_weights()\n",
        "        # self.device = \"cpu\"\n",
        "\n",
        "    def init_weights(self):\n",
        "        nn.init.uniform_(self.transitions, -0.1, 0.1)\n",
        "\n",
        "    def forward(self, W):\n",
        "        \"\"\"Decode each sentence within W and return predicted tagging.\n",
        "\n",
        "        Parameters\n",
        "        ---\n",
        "        W : torch.tensor\n",
        "            Word sequences of dimension batch size x max sentence length within batch + 2.\n",
        "\n",
        "        Returns\n",
        "        ---\n",
        "        sequences : list\n",
        "            List of tensors, each of which contains the predicted tag indices for a particular\n",
        "            word sequence.\n",
        "        \"\"\"\n",
        "        # Calculate scores.\n",
        "        emissions = self.calculate_emissions(W)\n",
        "        # Run viterbi sentence by sentence.\n",
        "        sequences = []\n",
        "        for sentence in range(W.shape[0]):\n",
        "            # Exclude beginning and end markers from each word sequence.\n",
        "            scores, backpointers = self.backward_viterbi_log(\n",
        "                W[sentence, 1:], emissions[sentence, :]\n",
        "            )\n",
        "            sequences += [self.get_viterbi(backpointers)]\n",
        "        return sequences\n",
        "\n",
        "    def calculate_emissions(self, W):\n",
        "        \"\"\"Calculate emissions (i.e., scores for each word and batch).\n",
        "\n",
        "        Parameters\n",
        "        ---\n",
        "        W : torch.tensor\n",
        "            Word sequences of dimension batch size x max sentence\n",
        "            length within batch + 2.\n",
        "\n",
        "        Returns\n",
        "        ---\n",
        "        emissions : torch.tensor\n",
        "            Word level scores for each tag of dimension batch_size x max\n",
        "            sentence length within batch + 1 x |T|.\n",
        "            The scores for the initial BOS index are already removed here\n",
        "            since we only needed it for the transformer.\n",
        "        \"\"\"\n",
        "        # Directly exclude emissions for the initial word in each sentence\n",
        "        # since these correspond to the BOS indices that we only need\n",
        "        # for BERT.\n",
        "        return self.emissions(self.transformer(W)[0])[:, 1:, :]\n",
        "\n",
        "    def loss(self, T, W):\n",
        "        \"\"\"Calculate the loss for a batch.\n",
        "\n",
        "        Parameters\n",
        "        ---\n",
        "        T : torch.tensor\n",
        "            True taggings for each sequence within the batch.\n",
        "            Of dimension batch size x longest sequence within batch + 2.\n",
        "            Note the paddings, EOS and BOS that have been added to T\n",
        "            for symmetry with W which needs this for BERT.\n",
        "        W : torch.tensor\n",
        "            Words for each sequence within the batch.\n",
        "            Of dimension batch size x longest sequence within batch + 2.\n",
        "            Note the paddings, EOS and BOS that have been added to W\n",
        "            for usage with BERT.\n",
        "\n",
        "        Returns\n",
        "        ---\n",
        "        torch.tensor\n",
        "            Mean loss for the batch.\n",
        "        \"\"\"\n",
        "        emissions = self.calculate_emissions(W)\n",
        "        # Note that we have to handle paddings and EOS within the score\n",
        "        # and backward functions, but we can already skip the BOS tokens\n",
        "        # here.\n",
        "        scores = self.score(emissions, W[:, 1:], T[:, 1:])\n",
        "        log_normalizer = self.backward_log_Z(W[:, 1:], emissions)\n",
        "        loss = torch.negative(torch.mean(scores - log_normalizer))\n",
        "        if self.beta > 0.0:\n",
        "            unnormalized_entropy = self.backward_entropy(\n",
        "                W[:, 1:], emissions\n",
        "            )\n",
        "            entropy = (\n",
        "                (unnormalized_entropy / torch.exp(log_normalizer))\n",
        "                + log_normalizer\n",
        "            )\n",
        "            return loss + torch.negative(self.beta * torch.mean(entropy))\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "    def score(self, emissions, W, T):\n",
        "        \"\"\"Calculate scores for specified taggings and word sequences.\n",
        "\n",
        "        Parameters\n",
        "        ---\n",
        "        emissions : torch.tensor\n",
        "        T : torch.tensor\n",
        "            Taggings for each sequence within the batch.\n",
        "            Of dimension batch size x longest sequence within batch + 1.\n",
        "            Note the paddings, EOS and BOS that have been added to T\n",
        "            for symmetry with W which needs this for BERT.\n",
        "            We expect T to already have the initial BOT tag indices removed\n",
        "            (see `loss` for details).\n",
        "        W : torch.tensor\n",
        "            Words for each sequence within the batch.\n",
        "            Of dimension batch size x longest sequence within batch + 1.\n",
        "            Note the paddings, EOS and BOS that have been added to W\n",
        "            for usage with BERT so we mask them out here. We expect\n",
        "            W to already have the initial BOS word indices taken out\n",
        "            (see `loss` for details).\n",
        "\n",
        "        Returns\n",
        "        ---\n",
        "        scores : torch.tensor\n",
        "            score(T, W) for all W[idx]s in W.\n",
        "        \"\"\"\n",
        "        scores = (\n",
        "            emissions[:, 0].gather(1, (T[:, 0]).unsqueeze(1)).squeeze()\n",
        "            + self.transitions[self.bot_idx, T[:, 0]]\n",
        "        )\n",
        "        for word in range(1, emissions.shape[1]):\n",
        "            mask = torch.where(\n",
        "                W[:, word] == self.pad_idx_word, 0, 1\n",
        "            ) * torch.where(W[:, word] == self.eos_idx, 0, 1)\n",
        "            scores += mask * (\n",
        "                emissions[:, word]\n",
        "                .gather(1, (T[:, word]).unsqueeze(1))\n",
        "                .squeeze()\n",
        "                + self.transitions[T[:, word - 1], T[:, word]]\n",
        "            )\n",
        "        return scores\n",
        "\n",
        "    def viterbi_naive(self, W, emissions):\n",
        "        \"\"\"Calculate best tagging naively and return both the best score and best tagging in log space.\n",
        "\n",
        "        NB: This naive version is not vectorized over W[idx]s.\n",
        "\n",
        "        Parameters\n",
        "        ---\n",
        "        W : torch.tensor\n",
        "            Of dimension longest sequence within batch + 2 or less.\n",
        "            Note the paddings, EOS and BOS that have been added to W\n",
        "            for usage with BERT so we manually remove them here if present.\n",
        "        emissions : torch.tensor\n",
        "            Word level scores for each tag of dimension max\n",
        "            sentence length within batch + 1 x |T| (we assume scores for the BOT\n",
        "            initial tag have already been removed since BOT/BOS is\n",
        "            only needed for the transformer).\n",
        "\n",
        "        Returns\n",
        "        ---\n",
        "        Tuple[torch.tensor, torch.tensor]\n",
        "            Tuple containing the log-score of the best tagging and the\n",
        "            indices of the best tagging for W.\n",
        "        \"\"\"\n",
        "        T = self.t_cal\n",
        "        # Remove padding.\n",
        "        if torch.any(W == self.pad_idx_word):\n",
        "            W = W[torch.where(W != self.pad_idx_word)[0]]\n",
        "        # Remove EOS and BOS if present.\n",
        "        if torch.any(W == self.eos_idx):\n",
        "            W = W[:-1]\n",
        "        if torch.any(W == self.bos_idx):\n",
        "            W = W[1:]\n",
        "        T_abs = len(T)\n",
        "        combinations = torch.combinations(\n",
        "            T, r=W.shape[0], with_replacement=True\n",
        "        )\n",
        "        combinations = torch.cartesian_prod(*[T for ix in range(W.shape[0])])\n",
        "        best_score = torch.tensor(0.0, dtype=torch.float64)\n",
        "        best_tag = torch.tensor([])\n",
        "        for ix, combination in enumerate(combinations):\n",
        "            if W.shape[0] == 1:\n",
        "                current_score = (\n",
        "                    emissions[0, combination]\n",
        "                    + self.transitions[self.bot_idx, combination]\n",
        "                )\n",
        "            else:\n",
        "                current_score = (\n",
        "                    emissions[0, combination[0]]\n",
        "                    + self.transitions[self.bot_idx, combination[0]]\n",
        "                )\n",
        "                for qx in range(1, combination.shape[0]):\n",
        "                    current_score += (\n",
        "                        emissions[qx, combination[qx]]\n",
        "                        + self.transitions[\n",
        "                            combination[qx - 1], combination[qx]\n",
        "                        ]\n",
        "                    )\n",
        "\n",
        "            if (current_score) > best_score:\n",
        "                best_score = current_score.double()\n",
        "                best_tag = combination\n",
        "        return best_score, best_tag\n",
        "\n",
        "    def log_Z_naive(self, W, emissions):\n",
        "        \"\"\"Calculate log Z naively.\n",
        "\n",
        "        NB: This naive version is not vectorized over W[idx]s.\n",
        "\n",
        "        Parameters\n",
        "        ---\n",
        "        W : torch.tensor\n",
        "            Of dimension longest sequence within batch + 2 or less.\n",
        "            Note the paddings, EOS and BOS that have been added to W\n",
        "            for usage with BERT so we manually remove them here if present.\n",
        "        emissions : torch.tensor\n",
        "            Word level scores for each tag of dimension max\n",
        "            sentence length within batch + 1 x |T| (we assume scores for the BOT\n",
        "            initial tag have already been removed since BOT/BOS is\n",
        "            only needed for the transformer).\n",
        "\n",
        "        Returns\n",
        "        ---\n",
        "        torch.tensor\n",
        "            Log Z for W.\n",
        "        \"\"\"\n",
        "        T = self.t_cal\n",
        "        # Remove padding\n",
        "        W = W[torch.where(W != self.pad_idx_word)[0]]\n",
        "        # Remove EOS and BOS if present\n",
        "        if torch.any(W == self.eos_idx):\n",
        "            W = W[:-1]\n",
        "        if torch.any(W == self.bos_idx):\n",
        "            W = W[1:]\n",
        "        T_abs = len(T)\n",
        "\n",
        "        # Generate \\mathcal{T}^N.\n",
        "        combinations = torch.cartesian_prod(*[T for ix in range(W.shape[0])])\n",
        "        log_normalizer = torch.zeros(\n",
        "            combinations.shape[0], dtype=torch.float64\n",
        "        )\n",
        "        # Loop over all possible combinations naively.\n",
        "        # NB: This is essentially line one on Slide 50.\n",
        "        for ix, combination in enumerate(combinations):\n",
        "            # Kludge since indexing is slightly different for one-dim\n",
        "            # tensors vs two tensors.\n",
        "            if W.shape[0] == 1:\n",
        "                # Calculate score as the sum of emissions (i.e., how well\n",
        "                # does a word match a tag based on BERT embeddings) and\n",
        "                # transitions (globally, how likely is a transition\n",
        "                # from the previous tag to the current tag).\n",
        "                # NB: For the first word, the initial tag is always BOT.\n",
        "                # NB 2: Since we are in log-space, the exp\n",
        "                # of the score goes away.\n",
        "                log_normalizer[ix] = (\n",
        "                    emissions[0, combination]\n",
        "                    + self.transitions[self.bot_idx, combination]\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                # Initial score is identical to above.\n",
        "                log_normalizer[ix] = (\n",
        "                    emissions[0, combination[0]]\n",
        "                    + self.transitions[self.bot_idx, combination[0]]\n",
        "                )\n",
        "                for qx in range(1, combination.shape[0]):\n",
        "                    # Score within each potential tagging\n",
        "                    # is calculated the same as above except that we now \n",
        "                    # actually use the previous tag instead of always\n",
        "                    # BOT.\n",
        "                    log_normalizer[ix] += (\n",
        "                        emissions[qx, combination[qx]]\n",
        "                        + self.transitions[\n",
        "                            combination[qx - 1], combination[qx]\n",
        "                        ]\n",
        "                    )\n",
        "        # Calculate logsumexp numerically stable\n",
        "        # since we are in log-space.\n",
        "        return torch.logsumexp(log_normalizer, 0)\n",
        "\n",
        "    def entropy_naive(self, W, emissions):\n",
        "        \"\"\"Calculate the unnormalized entropy naively.\n",
        "\n",
        "        NB: This naive version is not vectorized over W[idx]s.\n",
        "\n",
        "        Parameters\n",
        "        ---\n",
        "        W : torch.tensor\n",
        "            Words for each sequence within the batch.\n",
        "            Of dimension longest sequence within batch + 2 or less.\n",
        "            Note the paddings, EOS and BOS that have been added to W\n",
        "            for usage with BERT so we manually remove them here if present.\n",
        "        emissions : torch.tensor\n",
        "            Word level scores for each tag of dimension max\n",
        "            sentence length within batch + 1 x |T| (we assume scores for the BOT\n",
        "            initial tag have already been removed since BOT/BOS is\n",
        "            only needed for the transformer).\n",
        "\n",
        "        Returns\n",
        "        ---\n",
        "        torch.tensor\n",
        "            Log Z for W.\n",
        "        \"\"\"\n",
        "        T = self.t_cal\n",
        "        # Remove padding\n",
        "        W = W[torch.where(W != self.pad_idx_word)[0]]\n",
        "        # Remove EOS and BOS if present\n",
        "        if torch.any(W == self.eos_idx):\n",
        "            W = W[:-1]\n",
        "        if torch.any(W == self.bos_idx):\n",
        "            W = W[1:]\n",
        "        T_abs = len(T)\n",
        "        combinations = torch.combinations(\n",
        "            T, r=W.shape[0], with_replacement=True\n",
        "        )\n",
        "        combinations = torch.cartesian_prod(T, T)\n",
        "        combinations = torch.cartesian_prod(*[T for ix in range(W.shape[0])])\n",
        "        entropy = torch.zeros(1, dtype=torch.float64)\n",
        "        for ix, combination in enumerate(combinations):\n",
        "            if W.shape[0] == 1:\n",
        "                entropy -= torch.exp((\n",
        "                    emissions[0, combination]\n",
        "                    + self.transitions[self.bot_idx, combination]\n",
        "                )) * (\n",
        "                    emissions[0, combination]\n",
        "                    + self.transitions[self.bot_idx, combination]\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                local_score = (\n",
        "                    emissions[0, combination[0]]\n",
        "                    + self.transitions[self.bot_idx, combination[0]]\n",
        "                )\n",
        "                for qx in range(1, combination.shape[0]):\n",
        "                    local_score += (\n",
        "                        emissions[qx, combination[qx]]\n",
        "                        + self.transitions[\n",
        "                            combination[qx - 1], combination[qx]\n",
        "                        ]\n",
        "                    )\n",
        "                entropy -= torch.exp(local_score) * local_score\n",
        "        return entropy\n",
        "\n",
        "    def get_viterbi(self, backpointer_matrix):\n",
        "        \"\"\"Return the best tagging based on a backpointer matrix.\n",
        "\n",
        "        Parameters\n",
        "        ---\n",
        "        backpointer_matrix : torch.tensor\n",
        "            Backpointer matrix from Viterbi indicating which\n",
        "            tag is the highest scoring for each element in the sequence.\n",
        "\n",
        "        Returns\n",
        "        ---\n",
        "        torch.tensor\n",
        "            Indices of the best tagging based on `backpointer_matrix`.\n",
        "        \"\"\"\n",
        "        N = backpointer_matrix.shape[0]\n",
        "        tagging = torch.zeros(N, dtype=torch.int)\n",
        "\n",
        "        next_tag = 0\n",
        "        for i in range(0, N):\n",
        "          next_tag = backpointer_matrix[i, next_tag]\n",
        "          tagging[i] = next_tag\n",
        "\n",
        "        return tagging\n",
        "\n",
        "    def backward_log_Z(self, W, emissions):\n",
        "        \"\"\"Calculate log Z using the backward algorithm.\n",
        "\n",
        "        NB: You do need to vectorize this over W[idx]s.\n",
        "\n",
        "        Parameters\n",
        "        ---\n",
        "        W : torch.tensor\n",
        "            Words for each sequence within the batch.\n",
        "            Of dimension batch size x longest sequence within batch + 1.\n",
        "            Note the paddings, EOS and BOS that have been added to W\n",
        "            for usage with BERT so we mask them out here. We expect\n",
        "            W to already have the initial BOS word indices taken out.\n",
        "        emissions : torch.tensor\n",
        "            Word level scores for each tag of dimension batch_size x max\n",
        "            sentence length within batch + 1 x |T| (scores for the BOS\n",
        "            initial tag have already been removed since BOS is\n",
        "            only needed for the transformer).\n",
        "\n",
        "        Returns\n",
        "        ---\n",
        "        torch.tensor\n",
        "            Log Z for each sample in W.\n",
        "        \"\"\"\n",
        "        T = self.t_cal\n",
        "        \n",
        "        normalizer = torch.zeros(\n",
        "              [W.shape[0], W.shape[1]+1, T.shape[0]], dtype=torch.float64\n",
        "          )\n",
        "        \n",
        "        for i in range(W.shape[0]):\n",
        "            # Remove padding\n",
        "            idx = torch.where(W[i] == self.eos_idx)[0]\n",
        "            # Remove EOS and BOS if present\n",
        "            if torch.any(idx):\n",
        "                # print(f\"EOS at {idx}\")\n",
        "                normalizer[i, idx, :] = 1\n",
        "            else:\n",
        "                # print(\"No EOS\")\n",
        "                normalizer[i, -1, :] = 1\n",
        "        \n",
        "        N = W.shape[0]\n",
        "        for i in reversed(range(W.shape[1])):\n",
        "            for t1 in T:\n",
        "                normalizer[:, i, t1] += torch.sum(torch.exp(emissions[:, i, :] + self.transitions[None, t1, :]) * normalizer[:, i+1, :].clone(), 1)\n",
        "                # for t2 in T:\n",
        "                #     normalizer[:, i, t1] += torch.exp(emissions[:, i, t2] + self.transitions[t1, t2]) * normalizer[:, i+1, t2].clone()\n",
        "\n",
        "        #print(torch.log(normalizer))\n",
        "        return torch.log(normalizer[:, 0, 0])\n",
        "\n",
        "    def forward_log_Z(self, W, emissions):\n",
        "        \"\"\"Calculate log Z using the forward algorithm.\n",
        "\n",
        "        NB: You do need to vectorize this over samples.\n",
        "\n",
        "        Parameters\n",
        "        ---\n",
        "        W : torch.tensor\n",
        "            Words for each sequence within the batch.\n",
        "            Of dimension batch size x longest sequence within batch + 1.\n",
        "            Note the paddings, EOS and BOS that have been added to W\n",
        "            for usage with BERT so we mask them out here. We expect\n",
        "            W to already have the initial BOS word indices taken out.\n",
        "        emissions : torch.tensor\n",
        "            Word level scores for each tag of dimension batch_size x max\n",
        "            sentence length within batch + 1 x |T| (scores for the BOS\n",
        "            initial tag have already been removed since BOS is\n",
        "            only needed for the transformer).\n",
        "\n",
        "        Returns\n",
        "        ---\n",
        "        torch.tensor\n",
        "            Log Z for each sample in W.\n",
        "        \"\"\"\n",
        "        T = self.t_cal\n",
        "        \n",
        "        normalizer = torch.ones(\n",
        "              [W.shape[0]], dtype=torch.float64\n",
        "          )\n",
        "        \n",
        "        for idx, sample in enumerate(W):\n",
        "          # Remove padding\n",
        "          sample = sample[torch.where(sample != self.pad_idx_word)[0]]\n",
        "          # Remove EOS and BOS if present\n",
        "          if torch.any(sample == self.eos_idx):\n",
        "              sample = sample[:-1]\n",
        "          if torch.any(sample == self.bos_idx):\n",
        "              sample = sample[1:]\n",
        "\n",
        "          last_round = torch.ones(T.shape[0], dtype=torch.float64)\n",
        "\n",
        "          for t in T:\n",
        "            last_round[t] = torch.exp(emissions[idx, 0, t] + self.transitions[0, t])\n",
        "\n",
        "          N = sample.shape[0]\n",
        "          for i in range(1, N):\n",
        "\n",
        "            temp = torch.zeros(T.shape[0], dtype=torch.float64)\n",
        "\n",
        "            for t1 in T:\n",
        "              for t2 in T:\n",
        "                temp[t1] += torch.exp(emissions[idx, i, t1] + self.transitions[t2, t1]) * last_round[t2]\n",
        "\n",
        "            last_round = temp\n",
        "\n",
        "          normalizer[idx] = torch.sum(last_round)\n",
        "        #print(torch.log(normalizer))\n",
        "        return torch.log(normalizer)\n",
        "\n",
        "    def backward_entropy(self, W, emissions):\n",
        "        \"\"\"Calculate the unnormalized entropy using the backward algorithm.\n",
        "\n",
        "        NB: You do need to vectorize this over samples.\n",
        "\n",
        "        Parameters\n",
        "        ---\n",
        "        W : torch.tensor\n",
        "            Words for each sequence within the batch.\n",
        "            Of dimension batch size x longest sequence within batch + 1.\n",
        "            Note the paddings, EOS and BOS that have been added to W\n",
        "            for usage with BERT so we mask them out here. We expect\n",
        "            W to already have the initial BOS word indices taken out.\n",
        "        emissions : torch.tensor\n",
        "            Word level scores for each tag of dimension batch_size x max\n",
        "            sentence length within batch + 1 x |T| (scores for the EOS\n",
        "            initial tag have already been removed since EOS is\n",
        "            only needed for the transformer).\n",
        "\n",
        "        Returns\n",
        "        ---\n",
        "        torch.tensor\n",
        "            Unnormalized entropy for each sample in W.\n",
        "        \"\"\"\n",
        "        T = self.t_cal\n",
        "        \n",
        "        beta = torch.zeros([W.shape[0], W.shape[1]+1, T.shape[0], 2], dtype=torch.float64)\n",
        "        \n",
        "        for i in range(W.shape[0]):\n",
        "            idx = torch.where(W[i] == self.eos_idx)[0]\n",
        "            if torch.any(idx):\n",
        "                # print(f\"EOS at {idx}\")\n",
        "                beta[i, idx, :, 0] = 1\n",
        "                beta[i, idx, :, 1] = 0\n",
        "            else:\n",
        "                # print(\"No EOS\")\n",
        "                beta[i, -1, :, 0] = 1\n",
        "                beta[i, -1, :, 1] = 0\n",
        "        \n",
        "        # with open(\"beta.txt\", \"w\") as f:\n",
        "        #     for i in range(W.shape[1] + 1):\n",
        "        #         f.write(f\"beta start ({i})=\\n{beta[0, i, :, 0]}, {beta[0, i, :, 1]}\\n\")\n",
        "        \n",
        "        emissions.type(torch.float64)\n",
        "        self.transitions.type(torch.float64)\n",
        "        for i in reversed(range(W.shape[1])):\n",
        "            for t1 in T:\n",
        "                w = torch.exp(emissions[:, i, :] + self.transitions[None, t1, :])\n",
        "                y = -w * torch.log(w)\n",
        "                beta[:, i, t1, 0] += torch.sum(w * beta[:, i+1, :, 0].clone(), 1)\n",
        "                beta[:, i, t1, 1] += torch.sum(w * beta[:, i+1, :, 1].clone() + y * beta[:, i+1, :, 0].clone(), 1)\n",
        "                \n",
        "            # with open(\"beta.txt\", \"a\") as f:\n",
        "            #     f.write(f\"beta[{i}] {beta.shape}=\\n{beta[0, i, :, 0]}, {beta[0, i, :, 1]}\\n\")\n",
        "\n",
        "        # print(beta[0, :, :, :])\n",
        "        return beta[:, 0, 0, 1]\n",
        "          \n",
        "    def backward_viterbi_log(self, W, emissions):\n",
        "        \"\"\"Calculate the best tagging using the backward algorithm and return\n",
        "            both the scoring matrix in log-space and the backpointer matrix.\n",
        "\n",
        "        NB: You do not need to vectorize this over samples.\n",
        "\n",
        "        Parameters\n",
        "        ---\n",
        "        W : torch.tensor\n",
        "            Of dimension longest sequence within batch + 2 or less.\n",
        "            Note the paddings, EOS and BOS that have been added to W\n",
        "            for usage with BERT so we manually remove them here if present.\n",
        "        emissions : torch.tensor\n",
        "            Word level scores for each tag of dimension max\n",
        "            sentence length within batch + 1 x |T| (we assume scores for the BOT\n",
        "            initial tag have already been removed since BOT/BOS is\n",
        "            only needed for the transformer).\n",
        "\n",
        "        Returns\n",
        "        ---\n",
        "        Tuple[torch.tensor, torch.tensor]\n",
        "            Tuple containing the scoring matrix in log-space and the\n",
        "            backpointer matrix for recovering the best tagging.\n",
        "        \"\"\"\n",
        "        T = self.t_cal\n",
        "        \n",
        "        # Remove padding\n",
        "        W = W[torch.where(W != self.pad_idx_word)[0]]\n",
        "        # Remove EOS and BOS if present\n",
        "        if torch.any(W == self.eos_idx):\n",
        "            W = W[:-1]\n",
        "        if torch.any(W == self.bos_idx):\n",
        "            W = W[1:]\n",
        "\n",
        "        N = W.shape[0]\n",
        "        vit = torch.ones(\n",
        "              [N+1, T.shape[0]], dtype=torch.float64\n",
        "          )\n",
        "\n",
        "        bpm = torch.zeros([N, T.shape[0]], dtype=torch.int)\n",
        "        for i in reversed(range(0, N)):\n",
        "\n",
        "          temp = torch.ones(T.shape[0], dtype=torch.float64)\n",
        "\n",
        "          for t1 in T:\n",
        "            max = torch.zeros(1, dtype=torch.float64)\n",
        "            parent = -1\n",
        "            for t2 in T:\n",
        "              if max < torch.exp(emissions[i, t2] + self.transitions[t1, t2]) * vit[i+1, t2]:\n",
        "                max = torch.exp(emissions[i, t2] + self.transitions[t1, t2])  * vit[i+1, t2]\n",
        "                parent = t2\n",
        "            \n",
        "            bpm[i, t1] = parent\n",
        "            temp[t1] = max\n",
        "\n",
        "          vit[i] = temp\n",
        "        return [torch.log(vit[:-1]), bpm]\n",
        "\n",
        "    def dijkstra_viterbi_log(self, W, emissions):\n",
        "        \"\"\"Calculate the best tagging using Dijsktra's algorithm and return\n",
        "            both the best score and best tagging in log space.\n",
        "\n",
        "        NB: You do not need to vectorize this over samples.\n",
        "\n",
        "        Parameters\n",
        "        ---\n",
        "        W : torch.tensor\n",
        "            Of dimension longest sequence within batch + 2 or less.\n",
        "            Note the paddings, EOS and BOS that have been added to W\n",
        "            for usage with BERT so we manually remove them here if present.\n",
        "        emissions : torch.tensor\n",
        "            Word level scores for each tag of dimension max\n",
        "            sentence length within batch + 1 x |T| (we assume scores for the BOT\n",
        "            initial tag have already been removed since BOT/BOS is\n",
        "            only needed for the transformer).\n",
        "\n",
        "\n",
        "        Returns\n",
        "        ---\n",
        "        Tuple[torch.tensor, None, log_Z]\n",
        "            Tuple containing the log-score of the best tagging. \n",
        "            NB: Since there were some changes in the assignment,\n",
        "            we don't expect you to return the backpointer matrix\n",
        "            this year.\n",
        "            NB 2: We return log_Z if we already use it within the method\n",
        "            to calculate probabilities, such that we don't have to\n",
        "        \"\"\"\n",
        "        T = self.t_cal\n",
        "\n",
        "        \n",
        "        log_Z = self.backward_log_Z(W.unsqueeze(0), emissions.unsqueeze(0))\n",
        "        #print(log_Z)\n",
        "\n",
        "        #print(\"bare:\")\n",
        "        #print(W)\n",
        "        # Remove padding\n",
        "        W = W[torch.where(W != self.pad_idx_word)[0]]\n",
        "        # Remove EOS and BOS if present\n",
        "        if torch.any(W == self.eos_idx):\n",
        "            W = W[:-1]\n",
        "        if torch.any(W == self.bos_idx):\n",
        "            W = W[1:]\n",
        "        #print(\"trimmed:\")\n",
        "        #print(W)\n",
        "\n",
        "\n",
        "        g = torch.zeros([W.shape[0]+1, T.shape[0]], dtype=torch.float64)\n",
        "        #print(f\"g: {g.shape}\")\n",
        "        pq = PriorityQueue()\n",
        "        popped = set()\n",
        "\n",
        "        pq.push((0, (self.bot_idx, 0)))\n",
        "        #print(f\"push:   0, (0, {self.bot_idx})\")\n",
        "  \n",
        "        while(len(pq) > 0):\n",
        "            p = pq.pop()\n",
        "            score = p[0]\n",
        "            n = p[1][0]\n",
        "            t1 = p[1][1]\n",
        "            #print(f\"pop:   {score}, ({n}, {t1})\")\n",
        "            #print(f\"score:  {score}\")\n",
        "            #print(f\"n:      {n}\")\n",
        "            #print(f\"t1:     {t1}\")\n",
        "\n",
        "            popped.add((n, t1))\n",
        "            g[n, t1] = score\n",
        "\n",
        "            if n < W.shape[0]:\n",
        "                for t2 in T:\n",
        "                    if (n+1, t2) not in popped:\n",
        "                        #print(emissions[n, t2])\n",
        "                        new_score = emissions[n, t2] + self.transitions[t1, t2] - log_Z + g[n, t1]\n",
        "                        #print(new_score)\n",
        "                        pq.push((new_score, (n+1, t2.item())))\n",
        "                        #print(f\"push:   {new_score}, ({n+1}, {t2})\")\n",
        "        \n",
        "        #print(g)\n",
        "        return [torch.max(g[W.shape[0], :]), None, log_Z]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quoz0jBNb26e",
        "outputId": "3bf15da5-77e4-4b44-f0d6-ab4b9601c7f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "for i, data in enumerate(train_dataloader):\n",
        "    W_train = F.to_tensor(data[\"words\"], padding_value=pad_token_idx)\n",
        "    T_train = F.to_tensor(data[\"pos\"], padding_value=pos_vocab[pad_token])\n",
        "    if i == 0:\n",
        "        break\n",
        "\n",
        "bert = BertModel.from_pretrained(TRANSFORMER)\n",
        "T_CAL = torch.tensor([i for i in range(pos_vocab.__len__())])\n",
        "crf = NeuralCRF(\n",
        "    pad_idx_word=pad_token_idx,\n",
        "    pad_idx_pos=pos_vocab[pad_token],\n",
        "    bos_idx=init_token_idx,\n",
        "    eos_idx=sep_token_idx,\n",
        "    bot_idx=pos_vocab[init_token],\n",
        "    eot_idx=pos_vocab[sep_token],\n",
        "    t_cal=T_CAL,\n",
        "    transformer=bert,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl-a_j9oemGI"
      },
      "source": [
        "## Q3a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_MqYdBcMeko3"
      },
      "outputs": [],
      "source": [
        "emissions = crf.calculate_emissions(W_train)\n",
        "\n",
        "for sentence in range(W_train.shape[0]):\n",
        "    for word_index in [2, 3, 4]:\n",
        "        assert torch.isclose(\n",
        "            crf.log_Z_naive(\n",
        "                W_train[sentence, :word_index],\n",
        "                emissions[\n",
        "                    sentence,\n",
        "                ],\n",
        "            ),\n",
        "            crf.backward_log_Z(W_train[:, 1:word_index], emissions)[sentence],\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jfi7hVAKNfRF"
      },
      "outputs": [],
      "source": [
        "# NB: THIS TEST IS NOT GRADED! It is just to help you double check \n",
        "# your implementation of backward since the above test cases do not \n",
        "# contain instances of EOT/PAD.\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "crf = NeuralCRF(\n",
        "    pad_idx_word=pad_token_idx,\n",
        "    pad_idx_pos=pos_vocab[pad_token],\n",
        "    bos_idx=init_token_idx,\n",
        "    eos_idx=sep_token_idx,\n",
        "    bot_idx=pos_vocab[init_token],\n",
        "    eot_idx=pos_vocab[sep_token],\n",
        "    t_cal=T_CAL,\n",
        "    transformer=bert,\n",
        ")\n",
        "emissions = crf.calculate_emissions(W_train)\n",
        "assert torch.all(torch.isclose(crf.backward_log_Z(W_train[:, 1:], emissions), \n",
        "        \n",
        "        torch.tensor([ 88.3197,  54.9441,  51.8413,  49.4724, 110.4930,  40.2174,  40.0351,\n",
        "         49.2687, 107.3545,  60.9668,  58.0769,  89.1132,  57.9029,  51.7252,\n",
        "         83.1715,  79.5805,  82.7579,  33.7521,  61.2349,  27.4993,  45.7138,\n",
        "         49.2352,  52.6344, 122.5854, 150.2306, 156.5732,  40.0482,  30.7149,\n",
        "         27.3859,  92.0778,  61.3652,  80.0837], dtype=torch.float64)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdHg2VJUt5VY"
      },
      "source": [
        "## Q3b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "b9RkH9-4eqw2",
        "outputId": "1193eacb-07aa-4869-86c1-6f4b1b3fd6b2"
      },
      "outputs": [],
      "source": [
        "emissions = crf.calculate_emissions(W_train)\n",
        "\n",
        "assert torch.all(\n",
        "    torch.isclose(\n",
        "        crf.backward_log_Z(W_train[:, 1:], emissions),\n",
        "        crf.forward_log_Z(W_train[:, 1:], emissions),\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoqeybSRvDyC"
      },
      "source": [
        "## Q3c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HPHQf7ksetu8"
      },
      "outputs": [],
      "source": [
        "emissions = crf.calculate_emissions(W_train)\n",
        "\n",
        "for sentence in range(W_train.shape[0]):\n",
        "    for word_index in [2, 3, 4]:\n",
        "        score_naive, sequence_naive = crf.viterbi_naive(\n",
        "            W_train[sentence, :word_index],\n",
        "            emissions[\n",
        "                sentence,\n",
        "            ],\n",
        "        )\n",
        "        score_viterbi, backpointers_viterbi = crf.backward_viterbi_log(\n",
        "            W_train[sentence, :word_index],\n",
        "            emissions[\n",
        "                sentence,\n",
        "            ],\n",
        "        )\n",
        "        sequence_viterbi = crf.get_viterbi(backpointers_viterbi)\n",
        "        assert torch.isclose(score_viterbi[0, 0], score_naive)\n",
        "        assert torch.all(sequence_viterbi == sequence_naive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-4hs8vTzyLZD"
      },
      "outputs": [],
      "source": [
        "# NB: THIS TEST IS NOT GRADED! It is just to help you double check \n",
        "# your implementation of Viterbi since the above test cases do not \n",
        "# contain instances of EOT/PAD.\n",
        "# NB2: Our evaluation expects Viterbi to only predict tags for actual \n",
        "# words, thus Viterbi (or get_viterbi) is supposed to remove instances\n",
        "# of EOS/BOS/PAD. Example: If Viterbi is asked to predict the sequence \n",
        "# [\"BOS\", \"I\", \"like\" \"dogs\", \"EOS\", \"PAD\", \"PAD\"], it should return a tagging \n",
        "# of length 3 (one for each valid word in the input).\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "crf = NeuralCRF(\n",
        "    pad_idx_word=pad_token_idx,\n",
        "    pad_idx_pos=pos_vocab[pad_token],\n",
        "    bos_idx=init_token_idx,\n",
        "    eos_idx=sep_token_idx,\n",
        "    bot_idx=pos_vocab[init_token],\n",
        "    eot_idx=pos_vocab[sep_token],\n",
        "    t_cal=T_CAL,\n",
        "    transformer=bert,\n",
        ")\n",
        "emissions = crf.calculate_emissions(W_train)\n",
        "sequences_overall = []\n",
        "first_batch_sequences_test = [torch.tensor([10, 13, 10,  4, 13, 10, 10,  0,  0, 10, 10,  0, 10,  5,  4,  1,  5,  6,\n",
        "         10,  4,  0, 10,  0, 10,  0, 10,  0, 19, 15]),\n",
        " torch.tensor([10, 10,  4,  8, 15, 14,  4, 14, 10,  4, 14, 10,  4, 14,  4,  4,  5,  5]),\n",
        " torch.tensor([10,  4, 13, 10,  4, 13, 10,  4, 13, 10,  4, 10,  4, 10,  4, 13, 10]),\n",
        " torch.tensor([10, 10,  5, 10,  4,  9,  4, 10,  4, 15, 10,  1,  6, 10,  6, 10]),\n",
        " torch.tensor([10, 10, 10, 13, 10,  4, 10,  6, 10, 13,  5, 10, 12, 10, 12, 14, 14,  6,\n",
        "          6,  6, 10, 10, 10,  6, 10,  6, 10, 10,  6, 10,  7,  6, 10,  6, 18, 10]),\n",
        " torch.tensor([10,  4, 19, 15,  4, 15,  4, 15, 10,  0, 10,  4,  5]),\n",
        " torch.tensor([ 5,  5, 10, 10, 10,  6,  8, 15, 15, 10,  5, 10,  5]),\n",
        " torch.tensor([10,  4, 15, 14, 10,  4,  4, 15,  8, 10,  9,  4,  5,  5,  5,  5]),\n",
        " torch.tensor([10,  6, 10,  0, 10,  0, 10,  0, 10,  4,  0, 15, 10, 10,  0, 15,  5,  6,\n",
        "         10,  4, 15, 14, 10, 10,  6, 10, 10, 10, 10,  6, 10,  0, 10,  0, 10]),\n",
        " torch.tensor([10, 10, 10, 12, 10,  4,  2, 15,  8, 15, 14, 10,  6, 10,  6, 10,  6, 10,\n",
        "          4,  5]),\n",
        " torch.tensor([10, 13, 10,  4, 13, 10, 10,  5, 10,  6, 10,  4,  5,  5, 10,  5,  5, 10,\n",
        "          5]),\n",
        " torch.tensor([14, 13,  4, 15,  8, 15,  8, 10,  4,  4,  5, 10,  4,  5, 10,  5, 10,  6,\n",
        "         10, 17,  5,  5, 10,  4,  4, 19, 15,  0, 10]),\n",
        " torch.tensor([10,  4,  4, 15, 10,  4,  5, 10,  4,  5, 10,  4,  5, 10,  4,  4,  5,  5,\n",
        "          5]),\n",
        " torch.tensor([10,  8, 15, 14,  4,  5, 10,  4, 15,  8, 10,  4,  4, 15, 15, 15,  8]),\n",
        " torch.tensor([14, 10, 10, 10,  4, 17,  5, 10, 15, 14, 10,  0, 14,  4, 14, 10,  4,  4,\n",
        "         13, 15, 15, 10,  6,  8, 15,  8, 15]),\n",
        " torch.tensor([10, 10,  4,  1, 14, 10, 10,  4, 10,  4, 10,  4, 19, 19, 14,  4, 10,  4,\n",
        "         10,  4,  4,  4,  5, 10,  4, 15]),\n",
        " torch.tensor([15, 13, 10,  6,  8, 10,  4,  2, 15,  1, 15, 15, 10,  1, 14,  4, 15, 14,\n",
        "          5, 10, 10, 10, 10,  1,  7,  0, 15]),\n",
        " torch.tensor([10,  4,  0, 10, 10,  0, 10,  4, 13, 10, 10]),\n",
        " torch.tensor([10, 10, 13, 15, 14, 10, 15, 14,  4, 10,  6, 10, 10,  4, 13,  4, 13, 10,\n",
        "          4,  5]),\n",
        " torch.tensor([10,  4, 14,  5,  5,  5,  5,  5,  5]),\n",
        " torch.tensor([10, 10,  7, 10,  8, 15, 14, 10, 15, 14, 10,  5,  5,  5, 10]),\n",
        " torch.tensor([17,  5, 10, 10,  6, 10,  5,  5, 10,  4,  5,  4,  4,  5,  5,  5]),\n",
        " torch.tensor([10,  4, 15, 14, 10,  5,  6, 15,  8, 14, 10,  5, 10,  5,  6, 10,  5]),\n",
        " torch.tensor([10,  4,  4, 15,  8, 15,  7, 10,  4, 15, 14,  4, 13, 10,  4,  5,  4, 10,\n",
        "          5,  6,  5,  5, 10,  5,  6, 15,  8, 15,  8, 10,  0, 10,  4, 19, 15,  6,\n",
        "         10,  6, 10,  5]),\n",
        " torch.tensor([10,  4, 19, 15, 15,  7, 14,  4, 14,  4, 14, 18, 10,  4,  1, 15, 10, 10,\n",
        "         10, 10, 10, 10,  6,  6, 10, 10,  6, 10,  4,  6, 10,  6, 10,  6, 10,  0,\n",
        "         10, 10,  6, 10, 10,  8, 10,  9, 14, 10,  9,  5,  4]),\n",
        " torch.tensor([14, 10,  0, 10,  4,  0, 10,  4,  4,  5,  4,  4,  4,  4,  8, 10, 10,  0,\n",
        "         10,  4,  6, 10, 10,  6, 10,  4, 10,  4,  9,  5, 10,  4, 10,  0, 10,  0,\n",
        "         10,  6, 10, 10, 10,  0,  0, 10,  0, 10,  0, 10,  7, 10,  4]),\n",
        " torch.tensor([10,  6,  8, 15,  8, 15,  0, 15, 18,  1,  8, 15, 14]),\n",
        " torch.tensor([10,  4, 14, 11,  8, 18, 14, 14, 14,  5]),\n",
        " torch.tensor([10, 12, 10,  4,  8, 15, 14,  5,  5]),\n",
        " torch.tensor([10,  4, 10,  0, 10,  0, 10, 10,  0, 15, 10, 10, 10,  4, 10,  4,  0, 10,\n",
        "          0, 10,  0, 10,  0, 10,  4,  9, 10,  6, 10,  5]),\n",
        " torch.tensor([10, 10,  5, 10, 10,  0, 10,  5, 10,  5, 10,  5, 10,  5, 10,  4,  8, 15,\n",
        "          8, 17]),\n",
        " torch.tensor([ 8,  8, 10, 10, 10,  6, 10,  2,  8, 15, 10,  6, 10,  0,  8, 15, 14, 10,\n",
        "         10, 10,  2, 10,  6, 10, 18, 10])]\n",
        "\n",
        "for sentence in range(W_train.shape[0]):\n",
        "    score_viterbi, backpointers_viterbi = crf.backward_viterbi_log(\n",
        "        W_train[sentence, :],\n",
        "        emissions[\n",
        "            sentence,\n",
        "        ],\n",
        "    )\n",
        "    sequence_viterbi = crf.get_viterbi(backpointers_viterbi)\n",
        "    sequences_overall += [sequence_viterbi]\n",
        "\n",
        "assert torch.all(torch.tensor([torch.all(first_batch_sequences_test[ix] == sequences_overall[ix]) for ix in range(len(sequences_overall))]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwaRWIiVvLNQ"
      },
      "source": [
        "## Q3d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Yfch1WjMe1-u"
      },
      "outputs": [],
      "source": [
        "emissions = crf.calculate_emissions(W_train)\n",
        "\n",
        "for sentence in range(W_train.shape[0]):\n",
        "    for word_index in [2, 3, 4]:\n",
        "        score_viterbi, backpointers_viterbi = crf.backward_viterbi_log(\n",
        "            W_train[sentence, 1:word_index], emissions[sentence, :word_index]\n",
        "        )\n",
        "        #sequence_viterbi = crf.get_viterbi(backpointers_viterbi)\n",
        "        score_dijkstra, sequence_dijsktra, log_Z = crf.dijkstra_viterbi_log(\n",
        "            W_train[sentence, 1:word_index], emissions[sentence, :word_index]\n",
        "        )\n",
        "        #print(f\"sentence:   {W_train[sentence]}\")\n",
        "        #print(f\"sentence:   {sentence}\")\n",
        "        #print(f\"word_index: {word_index}\")\n",
        "        #print(f\"Viterbi:    {score_viterbi[0, 0]}\")\n",
        "        #print(f\"Dijkstra:   {score_dijkstra + torch.sum((W_train[sentence, 1:word_index] != crf.eos_idx) * (W_train[sentence, 1:word_index] != crf.pad_idx_word)) * log_Z}\")\n",
        "        assert torch.isclose(\n",
        "            score_viterbi[0, 0],\n",
        "                score_dijkstra\n",
        "                + torch.sum(\n",
        "                    (W_train[sentence, 1:word_index] != crf.eos_idx)\n",
        "                    * (W_train[sentence, 1:word_index] != crf.pad_idx_word)\n",
        "                )\n",
        "                * log_Z,\n",
        "        )\n",
        "        #assert torch.all(sequence_viterbi == sequence_dijsktra)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RknzYtrUvln7"
      },
      "source": [
        "## Q3e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49kT9v7yfO3f",
        "outputId": "54b6f7f1-603a-426e-f4b3-b5e238ffe582"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25.1 ms ± 330 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 10\n",
        "score_viterbi, backpointers_viterbi = crf.backward_viterbi_log(W_train[0, 1:4], emissions[0, :])\n",
        "#sequence_viterbi = crf.get_viterbi(backpointers_viterbi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cHJ0nUdfhVo",
        "outputId": "a96db8a2-1518-41a6-a174-751ea8746371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.8 ms ± 999 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 10\n",
        "score_dijkstra, sequence_dijsktra, log_Z = crf.dijkstra_viterbi_log(W_train[0, 1:4], emissions[0, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P8rf_32vo-x",
        "outputId": "33d54c46-c568-40b8-91a3-ac804b719005"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "289 ms ± 3.27 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 10\n",
        "score_naive, sequence_naive = crf.viterbi_naive(W_train[0, 1:4], emissions[0, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz5Gely7wU9X"
      },
      "source": [
        "## Q3f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ho8m3Vs-wVum"
      },
      "outputs": [],
      "source": [
        "def train_model_report_accuracy(\n",
        "    crf,\n",
        "    lr,\n",
        "    epochs,\n",
        "    train_dataloader,\n",
        "    dev_dataloader,\n",
        "    pad_token_idx_word,\n",
        "    pad_token_idx_tag,\n",
        "):\n",
        "\n",
        "    \"\"\"Train model for `epochs` epochs and report performance on \n",
        "        dev set after each epoch.\n",
        "\n",
        "    Parameters\n",
        "    ---\n",
        "    crf : NeuralCRF\n",
        "    lr : float\n",
        "        Learning rate to train with.\n",
        "    epochs : int\n",
        "        For how many epochs to train.\n",
        "    train_dataloader : torch.DataLoader\n",
        "    dev_dataloder : torch.DataLoader\n",
        "    pad_token_idx_word : int\n",
        "        Index with which to pad the word indices.\n",
        "    pad_token_idx_tag : int\n",
        "        Index with which to pad the tag indices.\n",
        "    \"\"\"\n",
        "    crf.train(True)\n",
        "    optimizer = torch.optim.Adam(crf.parameters(), lr=lr)\n",
        "    for epoch in range(epochs):\n",
        "        for i, data in enumerate(train_dataloader):\n",
        "            if i % 20 == 0:\n",
        "                print(f\"Epoch {epoch}, batch {i}\")\n",
        "            torch.autograd.set_detect_anomaly(True)\n",
        "            W = F.to_tensor(data[\"words\"], padding_value=pad_token_idx_word)\n",
        "            T = F.to_tensor(data[\"pos\"], padding_value=pad_token_idx_tag)\n",
        "            # W.to(crf.device)\n",
        "            # T.to(crf.device)\n",
        "            for param in crf.parameters():\n",
        "                param.grad = None\n",
        "            loss = crf.loss(T, W)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            predicted_sequences = []\n",
        "            true_sequences = []\n",
        "            for i_dev, data_dev in enumerate(valid_dataloader):\n",
        "                W_dev = F.to_tensor(\n",
        "                    data_dev[\"words\"], padding_value=pad_token_idx_word\n",
        "                )\n",
        "                T_dev = F.to_tensor(\n",
        "                    data_dev[\"pos\"], padding_value=pad_token_idx_tag\n",
        "                )\n",
        "                sequence_viterbi = crf(W_dev)\n",
        "                predicted_sequences += sequence_viterbi\n",
        "                for ix in range(W_dev.shape[0]):\n",
        "                    true_sequences += [\n",
        "                        T_dev[ix, 1 : (sequence_viterbi[ix].shape[0] + 1)]\n",
        "                    ]\n",
        "\n",
        "            acc = torch.tensor(0.0)\n",
        "            for ix in range(len(predicted_sequences)):\n",
        "                acc += torch.mean(\n",
        "                    (predicted_sequences[ix] == true_sequences[ix]).float()\n",
        "                )\n",
        "            acc = acc / len(predicted_sequences)\n",
        "            print(\"-------------------------\")\n",
        "            print(f\"Epoch: {epoch + 1} / {epochs}\")\n",
        "            print(f\"Development set accuracy: {acc}\")\n",
        "            print(\"-------------------------\")\n",
        "            pickle.dump(crf, open(f\"crf_epoch{epoch+1}\", 'wb'))\n",
        "        epoch += 1\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYrbXAELyRQR",
        "outputId": "c5bfbd18-0490-4925-e3fb-01f70fb7b0b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, batch 0\n",
            "Epoch 0, batch 1\n",
            "Epoch 0, batch 2\n",
            "Epoch 0, batch 3\n",
            "Epoch 0, batch 4\n",
            "Epoch 0, batch 5\n",
            "Epoch 0, batch 6\n",
            "Epoch 0, batch 7\n",
            "Epoch 0, batch 8\n",
            "Epoch 0, batch 9\n",
            "Epoch 0, batch 10\n",
            "Epoch 0, batch 11\n",
            "Epoch 0, batch 12\n",
            "Epoch 0, batch 13\n",
            "Epoch 0, batch 14\n",
            "Epoch 0, batch 15\n",
            "Epoch 0, batch 16\n",
            "Epoch 0, batch 17\n",
            "Epoch 0, batch 18\n",
            "Epoch 0, batch 19\n",
            "Epoch 0, batch 20\n",
            "Epoch 0, batch 21\n",
            "Epoch 0, batch 22\n",
            "Epoch 0, batch 23\n",
            "Epoch 0, batch 24\n",
            "Epoch 0, batch 25\n",
            "Epoch 0, batch 26\n",
            "Epoch 0, batch 27\n",
            "Epoch 0, batch 28\n",
            "Epoch 0, batch 29\n",
            "Epoch 0, batch 30\n",
            "Epoch 0, batch 31\n",
            "Epoch 0, batch 32\n",
            "Epoch 0, batch 33\n",
            "Epoch 0, batch 34\n",
            "Epoch 0, batch 35\n",
            "Epoch 0, batch 36\n",
            "Epoch 0, batch 37\n",
            "Epoch 0, batch 38\n",
            "Epoch 0, batch 39\n",
            "Epoch 0, batch 40\n",
            "Epoch 0, batch 41\n",
            "Epoch 0, batch 42\n",
            "Epoch 0, batch 43\n",
            "Epoch 0, batch 44\n",
            "Epoch 0, batch 45\n",
            "Epoch 0, batch 46\n",
            "Epoch 0, batch 47\n",
            "Epoch 0, batch 48\n",
            "Epoch 0, batch 49\n",
            "Epoch 0, batch 50\n",
            "Epoch 0, batch 51\n",
            "Epoch 0, batch 52\n",
            "Epoch 0, batch 53\n",
            "Epoch 0, batch 54\n",
            "Epoch 0, batch 55\n",
            "Epoch 0, batch 56\n",
            "Epoch 0, batch 57\n",
            "Epoch 0, batch 58\n",
            "Epoch 0, batch 59\n",
            "Epoch 0, batch 60\n",
            "Epoch 0, batch 61\n",
            "Epoch 0, batch 62\n",
            "Epoch 0, batch 63\n",
            "Epoch 0, batch 64\n",
            "Epoch 0, batch 65\n",
            "Epoch 0, batch 66\n",
            "Epoch 0, batch 67\n",
            "Epoch 0, batch 68\n",
            "Epoch 0, batch 69\n",
            "Epoch 0, batch 70\n",
            "Epoch 0, batch 71\n",
            "Epoch 0, batch 72\n",
            "Epoch 0, batch 73\n",
            "Epoch 0, batch 74\n",
            "Epoch 0, batch 75\n",
            "Epoch 0, batch 76\n",
            "Epoch 0, batch 77\n",
            "Epoch 0, batch 78\n",
            "Epoch 0, batch 79\n",
            "Epoch 0, batch 80\n",
            "Epoch 0, batch 81\n",
            "Epoch 0, batch 82\n",
            "Epoch 0, batch 83\n",
            "Epoch 0, batch 84\n",
            "Epoch 0, batch 85\n",
            "Epoch 0, batch 86\n",
            "Epoch 0, batch 87\n",
            "Epoch 0, batch 88\n",
            "Epoch 0, batch 89\n",
            "Epoch 0, batch 90\n",
            "Epoch 0, batch 91\n",
            "Epoch 0, batch 92\n",
            "Epoch 0, batch 93\n",
            "Epoch 0, batch 94\n",
            "Epoch 0, batch 95\n",
            "Epoch 0, batch 96\n",
            "Epoch 0, batch 97\n",
            "Epoch 0, batch 98\n",
            "Epoch 0, batch 99\n",
            "Epoch 0, batch 100\n",
            "Epoch 0, batch 101\n",
            "Epoch 0, batch 102\n",
            "Epoch 0, batch 103\n",
            "Epoch 0, batch 104\n",
            "Epoch 0, batch 105\n",
            "Epoch 0, batch 106\n",
            "Epoch 0, batch 107\n",
            "Epoch 0, batch 108\n",
            "Epoch 0, batch 109\n",
            "Epoch 0, batch 110\n",
            "Epoch 0, batch 111\n",
            "Epoch 0, batch 112\n",
            "Epoch 0, batch 113\n",
            "Epoch 0, batch 114\n",
            "Epoch 0, batch 115\n",
            "Epoch 0, batch 116\n",
            "Epoch 0, batch 117\n",
            "Epoch 0, batch 118\n",
            "Epoch 0, batch 119\n",
            "Epoch 0, batch 120\n",
            "Epoch 0, batch 121\n",
            "Epoch 0, batch 122\n",
            "Epoch 0, batch 123\n",
            "Epoch 0, batch 124\n",
            "Epoch 0, batch 125\n",
            "Epoch 0, batch 126\n",
            "Epoch 0, batch 127\n",
            "Epoch 0, batch 128\n",
            "Epoch 0, batch 129\n",
            "Epoch 0, batch 130\n",
            "Epoch 0, batch 131\n",
            "Epoch 0, batch 132\n",
            "Epoch 0, batch 133\n",
            "Epoch 0, batch 134\n",
            "Epoch 0, batch 135\n",
            "Epoch 0, batch 136\n",
            "Epoch 0, batch 137\n",
            "Epoch 0, batch 138\n",
            "Epoch 0, batch 139\n",
            "Epoch 0, batch 140\n",
            "Epoch 0, batch 141\n",
            "Epoch 0, batch 142\n",
            "Epoch 0, batch 143\n",
            "Epoch 0, batch 144\n",
            "Epoch 0, batch 145\n",
            "Epoch 0, batch 146\n",
            "Epoch 0, batch 147\n",
            "Epoch 0, batch 148\n",
            "Epoch 0, batch 149\n",
            "Epoch 0, batch 150\n",
            "Epoch 0, batch 151\n",
            "Epoch 0, batch 152\n",
            "Epoch 0, batch 153\n",
            "Epoch 0, batch 154\n",
            "Epoch 0, batch 155\n",
            "Epoch 0, batch 156\n",
            "Epoch 0, batch 157\n",
            "Epoch 0, batch 158\n",
            "Epoch 0, batch 159\n",
            "Epoch 0, batch 160\n",
            "Epoch 0, batch 161\n",
            "Epoch 0, batch 162\n",
            "Epoch 0, batch 163\n",
            "Epoch 0, batch 164\n",
            "Epoch 0, batch 165\n",
            "Epoch 0, batch 166\n",
            "Epoch 0, batch 167\n",
            "Epoch 0, batch 168\n",
            "Epoch 0, batch 169\n",
            "Epoch 0, batch 170\n",
            "Epoch 0, batch 171\n",
            "Epoch 0, batch 172\n",
            "Epoch 0, batch 173\n",
            "Epoch 0, batch 174\n",
            "Epoch 0, batch 175\n",
            "Epoch 0, batch 176\n",
            "Epoch 0, batch 177\n",
            "Epoch 0, batch 178\n",
            "Epoch 0, batch 179\n",
            "Epoch 0, batch 180\n",
            "Epoch 0, batch 181\n",
            "Epoch 0, batch 182\n",
            "Epoch 0, batch 183\n",
            "Epoch 0, batch 184\n",
            "Epoch 0, batch 185\n",
            "Epoch 0, batch 186\n",
            "Epoch 0, batch 187\n",
            "Epoch 0, batch 188\n",
            "Epoch 0, batch 189\n",
            "Epoch 0, batch 190\n",
            "Epoch 0, batch 191\n",
            "Epoch 0, batch 192\n",
            "Epoch 0, batch 193\n",
            "Epoch 0, batch 194\n",
            "Epoch 0, batch 195\n",
            "Epoch 0, batch 196\n",
            "Epoch 0, batch 197\n",
            "Epoch 0, batch 198\n",
            "Epoch 0, batch 199\n",
            "Epoch 0, batch 200\n",
            "Epoch 0, batch 201\n",
            "Epoch 0, batch 202\n",
            "Epoch 0, batch 203\n",
            "Epoch 0, batch 204\n",
            "Epoch 0, batch 205\n",
            "Epoch 0, batch 206\n",
            "Epoch 0, batch 207\n",
            "Epoch 0, batch 208\n",
            "Epoch 0, batch 209\n",
            "Epoch 0, batch 210\n",
            "Epoch 0, batch 211\n",
            "Epoch 0, batch 212\n",
            "Epoch 0, batch 213\n",
            "Epoch 0, batch 214\n",
            "Epoch 0, batch 215\n",
            "Epoch 0, batch 216\n",
            "Epoch 0, batch 217\n",
            "Epoch 0, batch 218\n",
            "Epoch 0, batch 219\n",
            "Epoch 0, batch 220\n",
            "Epoch 0, batch 221\n",
            "Epoch 0, batch 222\n",
            "Epoch 0, batch 223\n",
            "Epoch 0, batch 224\n",
            "Epoch 0, batch 225\n",
            "Epoch 0, batch 226\n",
            "Epoch 0, batch 227\n",
            "Epoch 0, batch 228\n",
            "Epoch 0, batch 229\n",
            "Epoch 0, batch 230\n",
            "Epoch 0, batch 231\n",
            "Epoch 0, batch 232\n",
            "Epoch 0, batch 233\n",
            "Epoch 0, batch 234\n",
            "Epoch 0, batch 235\n",
            "Epoch 0, batch 236\n",
            "Epoch 0, batch 237\n",
            "Epoch 0, batch 238\n",
            "Epoch 0, batch 239\n",
            "Epoch 0, batch 240\n",
            "Epoch 0, batch 241\n",
            "Epoch 0, batch 242\n",
            "Epoch 0, batch 243\n",
            "Epoch 0, batch 244\n",
            "Epoch 0, batch 245\n",
            "Epoch 0, batch 246\n",
            "Epoch 0, batch 247\n",
            "Epoch 0, batch 248\n",
            "Epoch 0, batch 249\n",
            "Epoch 0, batch 250\n",
            "Epoch 0, batch 251\n",
            "Epoch 0, batch 252\n",
            "Epoch 0, batch 253\n",
            "Epoch 0, batch 254\n",
            "Epoch 0, batch 255\n",
            "Epoch 0, batch 256\n",
            "Epoch 0, batch 257\n",
            "Epoch 0, batch 258\n",
            "Epoch 0, batch 259\n",
            "Epoch 0, batch 260\n",
            "Epoch 0, batch 261\n",
            "Epoch 0, batch 262\n",
            "Epoch 0, batch 263\n",
            "Epoch 0, batch 264\n",
            "Epoch 0, batch 265\n",
            "Epoch 0, batch 266\n",
            "Epoch 0, batch 267\n",
            "Epoch 0, batch 268\n",
            "Epoch 0, batch 269\n",
            "Epoch 0, batch 270\n",
            "Epoch 0, batch 271\n",
            "Epoch 0, batch 272\n",
            "Epoch 0, batch 273\n",
            "Epoch 0, batch 274\n",
            "Epoch 0, batch 275\n",
            "Epoch 0, batch 276\n",
            "Epoch 0, batch 277\n",
            "Epoch 0, batch 278\n",
            "Epoch 0, batch 279\n",
            "Epoch 0, batch 280\n",
            "Epoch 0, batch 281\n",
            "Epoch 0, batch 282\n",
            "Epoch 0, batch 283\n",
            "Epoch 0, batch 284\n",
            "Epoch 0, batch 285\n",
            "Epoch 0, batch 286\n",
            "Epoch 0, batch 287\n",
            "Epoch 0, batch 288\n",
            "Epoch 0, batch 289\n",
            "Epoch 0, batch 290\n",
            "Epoch 0, batch 291\n",
            "Epoch 0, batch 292\n",
            "Epoch 0, batch 293\n",
            "Epoch 0, batch 294\n",
            "Epoch 0, batch 295\n",
            "Epoch 0, batch 296\n",
            "Epoch 0, batch 297\n",
            "Epoch 0, batch 298\n",
            "Epoch 0, batch 299\n",
            "Epoch 0, batch 300\n",
            "Epoch 0, batch 301\n",
            "Epoch 0, batch 302\n",
            "Epoch 0, batch 303\n",
            "Epoch 0, batch 304\n",
            "Epoch 0, batch 305\n",
            "Epoch 0, batch 306\n",
            "Epoch 0, batch 307\n",
            "Epoch 0, batch 308\n",
            "Epoch 0, batch 309\n",
            "Epoch 0, batch 310\n",
            "Epoch 0, batch 311\n",
            "Epoch 0, batch 312\n",
            "Epoch 0, batch 313\n",
            "Epoch 0, batch 314\n",
            "Epoch 0, batch 315\n",
            "Epoch 0, batch 316\n",
            "Epoch 0, batch 317\n",
            "Epoch 0, batch 318\n",
            "Epoch 0, batch 319\n",
            "Epoch 0, batch 320\n",
            "Epoch 0, batch 321\n",
            "Epoch 0, batch 322\n",
            "Epoch 0, batch 323\n",
            "Epoch 0, batch 324\n",
            "Epoch 0, batch 325\n",
            "Epoch 0, batch 326\n",
            "Epoch 0, batch 327\n",
            "Epoch 0, batch 328\n",
            "Epoch 0, batch 329\n",
            "Epoch 0, batch 330\n",
            "Epoch 0, batch 331\n",
            "Epoch 0, batch 332\n",
            "Epoch 0, batch 333\n",
            "Epoch 0, batch 334\n",
            "Epoch 0, batch 335\n",
            "Epoch 0, batch 336\n",
            "Epoch 0, batch 337\n",
            "Epoch 0, batch 338\n",
            "Epoch 0, batch 339\n",
            "Epoch 0, batch 340\n",
            "Epoch 0, batch 341\n",
            "Epoch 0, batch 342\n",
            "Epoch 0, batch 343\n",
            "Epoch 0, batch 344\n",
            "Epoch 0, batch 345\n",
            "Epoch 0, batch 346\n",
            "Epoch 0, batch 347\n",
            "Epoch 0, batch 348\n",
            "Epoch 0, batch 349\n",
            "Epoch 0, batch 350\n",
            "Epoch 0, batch 351\n",
            "Epoch 0, batch 352\n",
            "Epoch 0, batch 353\n",
            "Epoch 0, batch 354\n",
            "Epoch 0, batch 355\n",
            "Epoch 0, batch 356\n",
            "Epoch 0, batch 357\n",
            "Epoch 0, batch 358\n",
            "Epoch 0, batch 359\n",
            "Epoch 0, batch 360\n",
            "Epoch 0, batch 361\n",
            "Epoch 0, batch 362\n",
            "Epoch 0, batch 363\n",
            "Epoch 0, batch 364\n",
            "Epoch 0, batch 365\n",
            "Epoch 0, batch 366\n",
            "Epoch 0, batch 367\n",
            "Epoch 0, batch 368\n",
            "Epoch 0, batch 369\n",
            "Epoch 0, batch 370\n",
            "Epoch 0, batch 371\n",
            "Epoch 0, batch 372\n",
            "Epoch 0, batch 373\n",
            "Epoch 0, batch 374\n",
            "Epoch 0, batch 375\n",
            "Epoch 0, batch 376\n",
            "Epoch 0, batch 377\n",
            "Epoch 0, batch 378\n",
            "Epoch 0, batch 379\n",
            "Epoch 0, batch 380\n",
            "Epoch 0, batch 381\n",
            "Epoch 0, batch 382\n",
            "Epoch 0, batch 383\n",
            "Epoch 0, batch 384\n",
            "Epoch 0, batch 385\n",
            "Epoch 0, batch 386\n",
            "Epoch 0, batch 387\n",
            "Epoch 0, batch 388\n",
            "Epoch 0, batch 389\n",
            "Epoch 0, batch 390\n",
            "Epoch 0, batch 391\n",
            "-------------------------\n",
            "Epoch: 1 / 3\n",
            "Development set accuracy: 0.8780471086502075\n",
            "-------------------------\n",
            "Epoch 1, batch 0\n",
            "Epoch 1, batch 1\n",
            "Epoch 1, batch 2\n",
            "Epoch 1, batch 3\n",
            "Epoch 1, batch 4\n",
            "Epoch 1, batch 5\n",
            "Epoch 1, batch 6\n",
            "Epoch 1, batch 7\n",
            "Epoch 1, batch 8\n",
            "Epoch 1, batch 9\n",
            "Epoch 1, batch 10\n",
            "Epoch 1, batch 11\n",
            "Epoch 1, batch 12\n",
            "Epoch 1, batch 13\n",
            "Epoch 1, batch 14\n",
            "Epoch 1, batch 15\n",
            "Epoch 1, batch 16\n",
            "Epoch 1, batch 17\n",
            "Epoch 1, batch 18\n",
            "Epoch 1, batch 19\n",
            "Epoch 1, batch 20\n",
            "Epoch 1, batch 21\n",
            "Epoch 1, batch 22\n",
            "Epoch 1, batch 23\n",
            "Epoch 1, batch 24\n",
            "Epoch 1, batch 25\n",
            "Epoch 1, batch 26\n",
            "Epoch 1, batch 27\n",
            "Epoch 1, batch 28\n",
            "Epoch 1, batch 29\n",
            "Epoch 1, batch 30\n",
            "Epoch 1, batch 31\n",
            "Epoch 1, batch 32\n",
            "Epoch 1, batch 33\n",
            "Epoch 1, batch 34\n",
            "Epoch 1, batch 35\n",
            "Epoch 1, batch 36\n",
            "Epoch 1, batch 37\n",
            "Epoch 1, batch 38\n",
            "Epoch 1, batch 39\n",
            "Epoch 1, batch 40\n",
            "Epoch 1, batch 41\n",
            "Epoch 1, batch 42\n",
            "Epoch 1, batch 43\n",
            "Epoch 1, batch 44\n",
            "Epoch 1, batch 45\n",
            "Epoch 1, batch 46\n",
            "Epoch 1, batch 47\n",
            "Epoch 1, batch 48\n",
            "Epoch 1, batch 49\n",
            "Epoch 1, batch 50\n",
            "Epoch 1, batch 51\n",
            "Epoch 1, batch 52\n",
            "Epoch 1, batch 53\n",
            "Epoch 1, batch 54\n",
            "Epoch 1, batch 55\n",
            "Epoch 1, batch 56\n",
            "Epoch 1, batch 57\n",
            "Epoch 1, batch 58\n",
            "Epoch 1, batch 59\n",
            "Epoch 1, batch 60\n",
            "Epoch 1, batch 61\n",
            "Epoch 1, batch 62\n",
            "Epoch 1, batch 63\n",
            "Epoch 1, batch 64\n",
            "Epoch 1, batch 65\n",
            "Epoch 1, batch 66\n",
            "Epoch 1, batch 67\n",
            "Epoch 1, batch 68\n",
            "Epoch 1, batch 69\n",
            "Epoch 1, batch 70\n",
            "Epoch 1, batch 71\n",
            "Epoch 1, batch 72\n",
            "Epoch 1, batch 73\n",
            "Epoch 1, batch 74\n",
            "Epoch 1, batch 75\n",
            "Epoch 1, batch 76\n",
            "Epoch 1, batch 77\n",
            "Epoch 1, batch 78\n",
            "Epoch 1, batch 79\n",
            "Epoch 1, batch 80\n",
            "Epoch 1, batch 81\n",
            "Epoch 1, batch 82\n",
            "Epoch 1, batch 83\n",
            "Epoch 1, batch 84\n",
            "Epoch 1, batch 85\n",
            "Epoch 1, batch 86\n",
            "Epoch 1, batch 87\n",
            "Epoch 1, batch 88\n",
            "Epoch 1, batch 89\n",
            "Epoch 1, batch 90\n",
            "Epoch 1, batch 91\n",
            "Epoch 1, batch 92\n",
            "Epoch 1, batch 93\n",
            "Epoch 1, batch 94\n",
            "Epoch 1, batch 95\n",
            "Epoch 1, batch 96\n",
            "Epoch 1, batch 97\n",
            "Epoch 1, batch 98\n",
            "Epoch 1, batch 99\n",
            "Epoch 1, batch 100\n",
            "Epoch 1, batch 101\n",
            "Epoch 1, batch 102\n",
            "Epoch 1, batch 103\n",
            "Epoch 1, batch 104\n",
            "Epoch 1, batch 105\n",
            "Epoch 1, batch 106\n",
            "Epoch 1, batch 107\n",
            "Epoch 1, batch 108\n",
            "Epoch 1, batch 109\n",
            "Epoch 1, batch 110\n",
            "Epoch 1, batch 111\n",
            "Epoch 1, batch 112\n",
            "Epoch 1, batch 113\n",
            "Epoch 1, batch 114\n",
            "Epoch 1, batch 115\n",
            "Epoch 1, batch 116\n",
            "Epoch 1, batch 117\n",
            "Epoch 1, batch 118\n",
            "Epoch 1, batch 119\n",
            "Epoch 1, batch 120\n",
            "Epoch 1, batch 121\n",
            "Epoch 1, batch 122\n",
            "Epoch 1, batch 123\n",
            "Epoch 1, batch 124\n",
            "Epoch 1, batch 125\n",
            "Epoch 1, batch 126\n",
            "Epoch 1, batch 127\n",
            "Epoch 1, batch 128\n",
            "Epoch 1, batch 129\n",
            "Epoch 1, batch 130\n",
            "Epoch 1, batch 131\n",
            "Epoch 1, batch 132\n",
            "Epoch 1, batch 133\n",
            "Epoch 1, batch 134\n",
            "Epoch 1, batch 135\n",
            "Epoch 1, batch 136\n",
            "Epoch 1, batch 137\n",
            "Epoch 1, batch 138\n",
            "Epoch 1, batch 139\n",
            "Epoch 1, batch 140\n",
            "Epoch 1, batch 141\n",
            "Epoch 1, batch 142\n",
            "Epoch 1, batch 143\n",
            "Epoch 1, batch 144\n",
            "Epoch 1, batch 145\n",
            "Epoch 1, batch 146\n",
            "Epoch 1, batch 147\n",
            "Epoch 1, batch 148\n",
            "Epoch 1, batch 149\n",
            "Epoch 1, batch 150\n",
            "Epoch 1, batch 151\n",
            "Epoch 1, batch 152\n",
            "Epoch 1, batch 153\n",
            "Epoch 1, batch 154\n",
            "Epoch 1, batch 155\n",
            "Epoch 1, batch 156\n",
            "Epoch 1, batch 157\n",
            "Epoch 1, batch 158\n",
            "Epoch 1, batch 159\n",
            "Epoch 1, batch 160\n",
            "Epoch 1, batch 161\n",
            "Epoch 1, batch 162\n",
            "Epoch 1, batch 163\n",
            "Epoch 1, batch 164\n",
            "Epoch 1, batch 165\n",
            "Epoch 1, batch 166\n",
            "Epoch 1, batch 167\n",
            "Epoch 1, batch 168\n",
            "Epoch 1, batch 169\n",
            "Epoch 1, batch 170\n",
            "Epoch 1, batch 171\n",
            "Epoch 1, batch 172\n",
            "Epoch 1, batch 173\n",
            "Epoch 1, batch 174\n",
            "Epoch 1, batch 175\n",
            "Epoch 1, batch 176\n",
            "Epoch 1, batch 177\n",
            "Epoch 1, batch 178\n",
            "Epoch 1, batch 179\n",
            "Epoch 1, batch 180\n",
            "Epoch 1, batch 181\n",
            "Epoch 1, batch 182\n",
            "Epoch 1, batch 183\n",
            "Epoch 1, batch 184\n",
            "Epoch 1, batch 185\n",
            "Epoch 1, batch 186\n",
            "Epoch 1, batch 187\n",
            "Epoch 1, batch 188\n",
            "Epoch 1, batch 189\n",
            "Epoch 1, batch 190\n",
            "Epoch 1, batch 191\n",
            "Epoch 1, batch 192\n",
            "Epoch 1, batch 193\n",
            "Epoch 1, batch 194\n",
            "Epoch 1, batch 195\n",
            "Epoch 1, batch 196\n",
            "Epoch 1, batch 197\n",
            "Epoch 1, batch 198\n",
            "Epoch 1, batch 199\n",
            "Epoch 1, batch 200\n",
            "Epoch 1, batch 201\n",
            "Epoch 1, batch 202\n",
            "Epoch 1, batch 203\n",
            "Epoch 1, batch 204\n",
            "Epoch 1, batch 205\n",
            "Epoch 1, batch 206\n",
            "Epoch 1, batch 207\n",
            "Epoch 1, batch 208\n",
            "Epoch 1, batch 209\n",
            "Epoch 1, batch 210\n",
            "Epoch 1, batch 211\n",
            "Epoch 1, batch 212\n",
            "Epoch 1, batch 213\n",
            "Epoch 1, batch 214\n",
            "Epoch 1, batch 215\n",
            "Epoch 1, batch 216\n",
            "Epoch 1, batch 217\n",
            "Epoch 1, batch 218\n",
            "Epoch 1, batch 219\n",
            "Epoch 1, batch 220\n",
            "Epoch 1, batch 221\n",
            "Epoch 1, batch 222\n",
            "Epoch 1, batch 223\n",
            "Epoch 1, batch 224\n",
            "Epoch 1, batch 225\n",
            "Epoch 1, batch 226\n",
            "Epoch 1, batch 227\n",
            "Epoch 1, batch 228\n",
            "Epoch 1, batch 229\n",
            "Epoch 1, batch 230\n",
            "Epoch 1, batch 231\n",
            "Epoch 1, batch 232\n",
            "Epoch 1, batch 233\n",
            "Epoch 1, batch 234\n",
            "Epoch 1, batch 235\n",
            "Epoch 1, batch 236\n",
            "Epoch 1, batch 237\n",
            "Epoch 1, batch 238\n",
            "Epoch 1, batch 239\n",
            "Epoch 1, batch 240\n",
            "Epoch 1, batch 241\n",
            "Epoch 1, batch 242\n",
            "Epoch 1, batch 243\n",
            "Epoch 1, batch 244\n",
            "Epoch 1, batch 245\n",
            "Epoch 1, batch 246\n",
            "Epoch 1, batch 247\n",
            "Epoch 1, batch 248\n",
            "Epoch 1, batch 249\n",
            "Epoch 1, batch 250\n",
            "Epoch 1, batch 251\n",
            "Epoch 1, batch 252\n",
            "Epoch 1, batch 253\n",
            "Epoch 1, batch 254\n",
            "Epoch 1, batch 255\n",
            "Epoch 1, batch 256\n",
            "Epoch 1, batch 257\n",
            "Epoch 1, batch 258\n",
            "Epoch 1, batch 259\n",
            "Epoch 1, batch 260\n",
            "Epoch 1, batch 261\n",
            "Epoch 1, batch 262\n",
            "Epoch 1, batch 263\n",
            "Epoch 1, batch 264\n",
            "Epoch 1, batch 265\n",
            "Epoch 1, batch 266\n",
            "Epoch 1, batch 267\n",
            "Epoch 1, batch 268\n",
            "Epoch 1, batch 269\n",
            "Epoch 1, batch 270\n",
            "Epoch 1, batch 271\n",
            "Epoch 1, batch 272\n",
            "Epoch 1, batch 273\n",
            "Epoch 1, batch 274\n",
            "Epoch 1, batch 275\n",
            "Epoch 1, batch 276\n",
            "Epoch 1, batch 277\n",
            "Epoch 1, batch 278\n",
            "Epoch 1, batch 279\n",
            "Epoch 1, batch 280\n",
            "Epoch 1, batch 281\n",
            "Epoch 1, batch 282\n",
            "Epoch 1, batch 283\n",
            "Epoch 1, batch 284\n",
            "Epoch 1, batch 285\n",
            "Epoch 1, batch 286\n",
            "Epoch 1, batch 287\n",
            "Epoch 1, batch 288\n",
            "Epoch 1, batch 289\n",
            "Epoch 1, batch 290\n",
            "Epoch 1, batch 291\n",
            "Epoch 1, batch 292\n",
            "Epoch 1, batch 293\n",
            "Epoch 1, batch 294\n",
            "Epoch 1, batch 295\n",
            "Epoch 1, batch 296\n",
            "Epoch 1, batch 297\n",
            "Epoch 1, batch 298\n",
            "Epoch 1, batch 299\n",
            "Epoch 1, batch 300\n",
            "Epoch 1, batch 301\n",
            "Epoch 1, batch 302\n",
            "Epoch 1, batch 303\n",
            "Epoch 1, batch 304\n",
            "Epoch 1, batch 305\n",
            "Epoch 1, batch 306\n",
            "Epoch 1, batch 307\n",
            "Epoch 1, batch 308\n",
            "Epoch 1, batch 309\n",
            "Epoch 1, batch 310\n",
            "Epoch 1, batch 311\n",
            "Epoch 1, batch 312\n",
            "Epoch 1, batch 313\n",
            "Epoch 1, batch 314\n",
            "Epoch 1, batch 315\n",
            "Epoch 1, batch 316\n",
            "Epoch 1, batch 317\n",
            "Epoch 1, batch 318\n",
            "Epoch 1, batch 319\n",
            "Epoch 1, batch 320\n",
            "Epoch 1, batch 321\n",
            "Epoch 1, batch 322\n",
            "Epoch 1, batch 323\n",
            "Epoch 1, batch 324\n",
            "Epoch 1, batch 325\n",
            "Epoch 1, batch 326\n",
            "Epoch 1, batch 327\n",
            "Epoch 1, batch 328\n",
            "Epoch 1, batch 329\n",
            "Epoch 1, batch 330\n",
            "Epoch 1, batch 331\n",
            "Epoch 1, batch 332\n",
            "Epoch 1, batch 333\n",
            "Epoch 1, batch 334\n",
            "Epoch 1, batch 335\n",
            "Epoch 1, batch 336\n",
            "Epoch 1, batch 337\n",
            "Epoch 1, batch 338\n",
            "Epoch 1, batch 339\n",
            "Epoch 1, batch 340\n",
            "Epoch 1, batch 341\n",
            "Epoch 1, batch 342\n",
            "Epoch 1, batch 343\n",
            "Epoch 1, batch 344\n",
            "Epoch 1, batch 345\n",
            "Epoch 1, batch 346\n",
            "Epoch 1, batch 347\n",
            "Epoch 1, batch 348\n",
            "Epoch 1, batch 349\n",
            "Epoch 1, batch 350\n",
            "Epoch 1, batch 351\n",
            "Epoch 1, batch 352\n",
            "Epoch 1, batch 353\n",
            "Epoch 1, batch 354\n",
            "Epoch 1, batch 355\n",
            "Epoch 1, batch 356\n",
            "Epoch 1, batch 357\n",
            "Epoch 1, batch 358\n",
            "Epoch 1, batch 359\n",
            "Epoch 1, batch 360\n",
            "Epoch 1, batch 361\n",
            "Epoch 1, batch 362\n",
            "Epoch 1, batch 363\n",
            "Epoch 1, batch 364\n",
            "Epoch 1, batch 365\n",
            "Epoch 1, batch 366\n",
            "Epoch 1, batch 367\n",
            "Epoch 1, batch 368\n",
            "Epoch 1, batch 369\n",
            "Epoch 1, batch 370\n",
            "Epoch 1, batch 371\n",
            "Epoch 1, batch 372\n",
            "Epoch 1, batch 373\n",
            "Epoch 1, batch 374\n",
            "Epoch 1, batch 375\n",
            "Epoch 1, batch 376\n",
            "Epoch 1, batch 377\n",
            "Epoch 1, batch 378\n",
            "Epoch 1, batch 379\n",
            "Epoch 1, batch 380\n",
            "Epoch 1, batch 381\n",
            "Epoch 1, batch 382\n",
            "Epoch 1, batch 383\n",
            "Epoch 1, batch 384\n",
            "Epoch 1, batch 385\n",
            "Epoch 1, batch 386\n",
            "Epoch 1, batch 387\n",
            "Epoch 1, batch 388\n",
            "Epoch 1, batch 389\n",
            "Epoch 1, batch 390\n",
            "Epoch 1, batch 391\n",
            "-------------------------\n",
            "Epoch: 2 / 3\n",
            "Development set accuracy: 0.898841917514801\n",
            "-------------------------\n",
            "Epoch 2, batch 0\n",
            "Epoch 2, batch 1\n",
            "Epoch 2, batch 2\n",
            "Epoch 2, batch 3\n",
            "Epoch 2, batch 4\n",
            "Epoch 2, batch 5\n",
            "Epoch 2, batch 6\n",
            "Epoch 2, batch 7\n",
            "Epoch 2, batch 8\n",
            "Epoch 2, batch 9\n",
            "Epoch 2, batch 10\n",
            "Epoch 2, batch 11\n",
            "Epoch 2, batch 12\n",
            "Epoch 2, batch 13\n",
            "Epoch 2, batch 14\n",
            "Epoch 2, batch 15\n",
            "Epoch 2, batch 16\n",
            "Epoch 2, batch 17\n",
            "Epoch 2, batch 18\n",
            "Epoch 2, batch 19\n",
            "Epoch 2, batch 20\n",
            "Epoch 2, batch 21\n",
            "Epoch 2, batch 22\n",
            "Epoch 2, batch 23\n",
            "Epoch 2, batch 24\n",
            "Epoch 2, batch 25\n",
            "Epoch 2, batch 26\n",
            "Epoch 2, batch 27\n",
            "Epoch 2, batch 28\n",
            "Epoch 2, batch 29\n",
            "Epoch 2, batch 30\n",
            "Epoch 2, batch 31\n",
            "Epoch 2, batch 32\n",
            "Epoch 2, batch 33\n",
            "Epoch 2, batch 34\n",
            "Epoch 2, batch 35\n",
            "Epoch 2, batch 36\n",
            "Epoch 2, batch 37\n",
            "Epoch 2, batch 38\n",
            "Epoch 2, batch 39\n",
            "Epoch 2, batch 40\n",
            "Epoch 2, batch 41\n",
            "Epoch 2, batch 42\n",
            "Epoch 2, batch 43\n",
            "Epoch 2, batch 44\n",
            "Epoch 2, batch 45\n",
            "Epoch 2, batch 46\n",
            "Epoch 2, batch 47\n",
            "Epoch 2, batch 48\n",
            "Epoch 2, batch 49\n",
            "Epoch 2, batch 50\n",
            "Epoch 2, batch 51\n",
            "Epoch 2, batch 52\n",
            "Epoch 2, batch 53\n",
            "Epoch 2, batch 54\n",
            "Epoch 2, batch 55\n",
            "Epoch 2, batch 56\n",
            "Epoch 2, batch 57\n",
            "Epoch 2, batch 58\n",
            "Epoch 2, batch 59\n",
            "Epoch 2, batch 60\n",
            "Epoch 2, batch 61\n",
            "Epoch 2, batch 62\n",
            "Epoch 2, batch 63\n",
            "Epoch 2, batch 64\n",
            "Epoch 2, batch 65\n",
            "Epoch 2, batch 66\n",
            "Epoch 2, batch 67\n",
            "Epoch 2, batch 68\n",
            "Epoch 2, batch 69\n",
            "Epoch 2, batch 70\n",
            "Epoch 2, batch 71\n",
            "Epoch 2, batch 72\n",
            "Epoch 2, batch 73\n",
            "Epoch 2, batch 74\n",
            "Epoch 2, batch 75\n",
            "Epoch 2, batch 76\n",
            "Epoch 2, batch 77\n",
            "Epoch 2, batch 78\n",
            "Epoch 2, batch 79\n",
            "Epoch 2, batch 80\n",
            "Epoch 2, batch 81\n",
            "Epoch 2, batch 82\n",
            "Epoch 2, batch 83\n",
            "Epoch 2, batch 84\n",
            "Epoch 2, batch 85\n",
            "Epoch 2, batch 86\n",
            "Epoch 2, batch 87\n",
            "Epoch 2, batch 88\n",
            "Epoch 2, batch 89\n",
            "Epoch 2, batch 90\n",
            "Epoch 2, batch 91\n",
            "Epoch 2, batch 92\n",
            "Epoch 2, batch 93\n",
            "Epoch 2, batch 94\n",
            "Epoch 2, batch 95\n",
            "Epoch 2, batch 96\n",
            "Epoch 2, batch 97\n",
            "Epoch 2, batch 98\n",
            "Epoch 2, batch 99\n",
            "Epoch 2, batch 100\n",
            "Epoch 2, batch 101\n",
            "Epoch 2, batch 102\n",
            "Epoch 2, batch 103\n",
            "Epoch 2, batch 104\n",
            "Epoch 2, batch 105\n",
            "Epoch 2, batch 106\n",
            "Epoch 2, batch 107\n",
            "Epoch 2, batch 108\n",
            "Epoch 2, batch 109\n",
            "Epoch 2, batch 110\n",
            "Epoch 2, batch 111\n",
            "Epoch 2, batch 112\n",
            "Epoch 2, batch 113\n",
            "Epoch 2, batch 114\n",
            "Epoch 2, batch 115\n",
            "Epoch 2, batch 116\n",
            "Epoch 2, batch 117\n",
            "Epoch 2, batch 118\n",
            "Epoch 2, batch 119\n",
            "Epoch 2, batch 120\n",
            "Epoch 2, batch 121\n",
            "Epoch 2, batch 122\n",
            "Epoch 2, batch 123\n",
            "Epoch 2, batch 124\n",
            "Epoch 2, batch 125\n",
            "Epoch 2, batch 126\n",
            "Epoch 2, batch 127\n",
            "Epoch 2, batch 128\n",
            "Epoch 2, batch 129\n",
            "Epoch 2, batch 130\n",
            "Epoch 2, batch 131\n",
            "Epoch 2, batch 132\n",
            "Epoch 2, batch 133\n",
            "Epoch 2, batch 134\n",
            "Epoch 2, batch 135\n",
            "Epoch 2, batch 136\n",
            "Epoch 2, batch 137\n",
            "Epoch 2, batch 138\n",
            "Epoch 2, batch 139\n",
            "Epoch 2, batch 140\n",
            "Epoch 2, batch 141\n",
            "Epoch 2, batch 142\n",
            "Epoch 2, batch 143\n",
            "Epoch 2, batch 144\n",
            "Epoch 2, batch 145\n",
            "Epoch 2, batch 146\n",
            "Epoch 2, batch 147\n",
            "Epoch 2, batch 148\n",
            "Epoch 2, batch 149\n",
            "Epoch 2, batch 150\n",
            "Epoch 2, batch 151\n",
            "Epoch 2, batch 152\n",
            "Epoch 2, batch 153\n",
            "Epoch 2, batch 154\n",
            "Epoch 2, batch 155\n",
            "Epoch 2, batch 156\n",
            "Epoch 2, batch 157\n",
            "Epoch 2, batch 158\n",
            "Epoch 2, batch 159\n",
            "Epoch 2, batch 160\n",
            "Epoch 2, batch 161\n",
            "Epoch 2, batch 162\n",
            "Epoch 2, batch 163\n",
            "Epoch 2, batch 164\n",
            "Epoch 2, batch 165\n",
            "Epoch 2, batch 166\n",
            "Epoch 2, batch 167\n",
            "Epoch 2, batch 168\n",
            "Epoch 2, batch 169\n",
            "Epoch 2, batch 170\n",
            "Epoch 2, batch 171\n",
            "Epoch 2, batch 172\n",
            "Epoch 2, batch 173\n",
            "Epoch 2, batch 174\n",
            "Epoch 2, batch 175\n",
            "Epoch 2, batch 176\n",
            "Epoch 2, batch 177\n",
            "Epoch 2, batch 178\n",
            "Epoch 2, batch 179\n",
            "Epoch 2, batch 180\n",
            "Epoch 2, batch 181\n",
            "Epoch 2, batch 182\n",
            "Epoch 2, batch 183\n",
            "Epoch 2, batch 184\n",
            "Epoch 2, batch 185\n",
            "Epoch 2, batch 186\n",
            "Epoch 2, batch 187\n",
            "Epoch 2, batch 188\n",
            "Epoch 2, batch 189\n",
            "Epoch 2, batch 190\n",
            "Epoch 2, batch 191\n",
            "Epoch 2, batch 192\n",
            "Epoch 2, batch 193\n",
            "Epoch 2, batch 194\n",
            "Epoch 2, batch 195\n",
            "Epoch 2, batch 196\n",
            "Epoch 2, batch 197\n",
            "Epoch 2, batch 198\n",
            "Epoch 2, batch 199\n",
            "Epoch 2, batch 200\n",
            "Epoch 2, batch 201\n",
            "Epoch 2, batch 202\n",
            "Epoch 2, batch 203\n",
            "Epoch 2, batch 204\n",
            "Epoch 2, batch 205\n",
            "Epoch 2, batch 206\n",
            "Epoch 2, batch 207\n",
            "Epoch 2, batch 208\n",
            "Epoch 2, batch 209\n",
            "Epoch 2, batch 210\n",
            "Epoch 2, batch 211\n",
            "Epoch 2, batch 212\n",
            "Epoch 2, batch 213\n",
            "Epoch 2, batch 214\n",
            "Epoch 2, batch 215\n",
            "Epoch 2, batch 216\n",
            "Epoch 2, batch 217\n",
            "Epoch 2, batch 218\n",
            "Epoch 2, batch 219\n",
            "Epoch 2, batch 220\n",
            "Epoch 2, batch 221\n",
            "Epoch 2, batch 222\n",
            "Epoch 2, batch 223\n",
            "Epoch 2, batch 224\n",
            "Epoch 2, batch 225\n",
            "Epoch 2, batch 226\n",
            "Epoch 2, batch 227\n",
            "Epoch 2, batch 228\n",
            "Epoch 2, batch 229\n",
            "Epoch 2, batch 230\n",
            "Epoch 2, batch 231\n",
            "Epoch 2, batch 232\n",
            "Epoch 2, batch 233\n",
            "Epoch 2, batch 234\n",
            "Epoch 2, batch 235\n",
            "Epoch 2, batch 236\n",
            "Epoch 2, batch 237\n",
            "Epoch 2, batch 238\n",
            "Epoch 2, batch 239\n",
            "Epoch 2, batch 240\n",
            "Epoch 2, batch 241\n",
            "Epoch 2, batch 242\n",
            "Epoch 2, batch 243\n",
            "Epoch 2, batch 244\n",
            "Epoch 2, batch 245\n",
            "Epoch 2, batch 246\n",
            "Epoch 2, batch 247\n",
            "Epoch 2, batch 248\n",
            "Epoch 2, batch 249\n",
            "Epoch 2, batch 250\n",
            "Epoch 2, batch 251\n",
            "Epoch 2, batch 252\n",
            "Epoch 2, batch 253\n",
            "Epoch 2, batch 254\n",
            "Epoch 2, batch 255\n",
            "Epoch 2, batch 256\n",
            "Epoch 2, batch 257\n",
            "Epoch 2, batch 258\n",
            "Epoch 2, batch 259\n",
            "Epoch 2, batch 260\n",
            "Epoch 2, batch 261\n",
            "Epoch 2, batch 262\n",
            "Epoch 2, batch 263\n",
            "Epoch 2, batch 264\n",
            "Epoch 2, batch 265\n",
            "Epoch 2, batch 266\n",
            "Epoch 2, batch 267\n",
            "Epoch 2, batch 268\n",
            "Epoch 2, batch 269\n",
            "Epoch 2, batch 270\n",
            "Epoch 2, batch 271\n",
            "Epoch 2, batch 272\n",
            "Epoch 2, batch 273\n",
            "Epoch 2, batch 274\n",
            "Epoch 2, batch 275\n",
            "Epoch 2, batch 276\n",
            "Epoch 2, batch 277\n",
            "Epoch 2, batch 278\n",
            "Epoch 2, batch 279\n",
            "Epoch 2, batch 280\n",
            "Epoch 2, batch 281\n",
            "Epoch 2, batch 282\n",
            "Epoch 2, batch 283\n",
            "Epoch 2, batch 284\n",
            "Epoch 2, batch 285\n",
            "Epoch 2, batch 286\n",
            "Epoch 2, batch 287\n",
            "Epoch 2, batch 288\n",
            "Epoch 2, batch 289\n",
            "Epoch 2, batch 290\n",
            "Epoch 2, batch 291\n",
            "Epoch 2, batch 292\n",
            "Epoch 2, batch 293\n",
            "Epoch 2, batch 294\n",
            "Epoch 2, batch 295\n",
            "Epoch 2, batch 296\n",
            "Epoch 2, batch 297\n",
            "Epoch 2, batch 298\n",
            "Epoch 2, batch 299\n",
            "Epoch 2, batch 300\n",
            "Epoch 2, batch 301\n",
            "Epoch 2, batch 302\n",
            "Epoch 2, batch 303\n",
            "Epoch 2, batch 304\n",
            "Epoch 2, batch 305\n",
            "Epoch 2, batch 306\n",
            "Epoch 2, batch 307\n",
            "Epoch 2, batch 308\n",
            "Epoch 2, batch 309\n",
            "Epoch 2, batch 310\n",
            "Epoch 2, batch 311\n",
            "Epoch 2, batch 312\n",
            "Epoch 2, batch 313\n",
            "Epoch 2, batch 314\n",
            "Epoch 2, batch 315\n",
            "Epoch 2, batch 316\n",
            "Epoch 2, batch 317\n",
            "Epoch 2, batch 318\n",
            "Epoch 2, batch 319\n",
            "Epoch 2, batch 320\n",
            "Epoch 2, batch 321\n",
            "Epoch 2, batch 322\n",
            "Epoch 2, batch 323\n",
            "Epoch 2, batch 324\n",
            "Epoch 2, batch 325\n",
            "Epoch 2, batch 326\n",
            "Epoch 2, batch 327\n",
            "Epoch 2, batch 328\n",
            "Epoch 2, batch 329\n",
            "Epoch 2, batch 330\n",
            "Epoch 2, batch 331\n",
            "Epoch 2, batch 332\n",
            "Epoch 2, batch 333\n",
            "Epoch 2, batch 334\n",
            "Epoch 2, batch 335\n",
            "Epoch 2, batch 336\n",
            "Epoch 2, batch 337\n",
            "Epoch 2, batch 338\n",
            "Epoch 2, batch 339\n",
            "Epoch 2, batch 340\n",
            "Epoch 2, batch 341\n",
            "Epoch 2, batch 342\n",
            "Epoch 2, batch 343\n",
            "Epoch 2, batch 344\n",
            "Epoch 2, batch 345\n",
            "Epoch 2, batch 346\n",
            "Epoch 2, batch 347\n",
            "Epoch 2, batch 348\n",
            "Epoch 2, batch 349\n",
            "Epoch 2, batch 350\n",
            "Epoch 2, batch 351\n",
            "Epoch 2, batch 352\n",
            "Epoch 2, batch 353\n",
            "Epoch 2, batch 354\n",
            "Epoch 2, batch 355\n",
            "Epoch 2, batch 356\n",
            "Epoch 2, batch 357\n",
            "Epoch 2, batch 358\n",
            "Epoch 2, batch 359\n",
            "Epoch 2, batch 360\n",
            "Epoch 2, batch 361\n",
            "Epoch 2, batch 362\n",
            "Epoch 2, batch 363\n",
            "Epoch 2, batch 364\n",
            "Epoch 2, batch 365\n",
            "Epoch 2, batch 366\n",
            "Epoch 2, batch 367\n",
            "Epoch 2, batch 368\n",
            "Epoch 2, batch 369\n",
            "Epoch 2, batch 370\n",
            "Epoch 2, batch 371\n",
            "Epoch 2, batch 372\n",
            "Epoch 2, batch 373\n",
            "Epoch 2, batch 374\n",
            "Epoch 2, batch 375\n",
            "Epoch 2, batch 376\n",
            "Epoch 2, batch 377\n",
            "Epoch 2, batch 378\n",
            "Epoch 2, batch 379\n",
            "Epoch 2, batch 380\n",
            "Epoch 2, batch 381\n",
            "Epoch 2, batch 382\n",
            "Epoch 2, batch 383\n",
            "Epoch 2, batch 384\n",
            "Epoch 2, batch 385\n",
            "Epoch 2, batch 386\n",
            "Epoch 2, batch 387\n",
            "Epoch 2, batch 388\n",
            "Epoch 2, batch 389\n",
            "Epoch 2, batch 390\n",
            "Epoch 2, batch 391\n",
            "-------------------------\n",
            "Epoch: 3 / 3\n",
            "Development set accuracy: 0.9058566093444824\n",
            "-------------------------\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "crf = NeuralCRF(\n",
        "    pad_idx_word=pad_token_idx,\n",
        "    pad_idx_pos=pos_vocab[pad_token],\n",
        "    bos_idx=init_token_idx,\n",
        "    eos_idx=sep_token_idx,\n",
        "    bot_idx=pos_vocab[init_token],\n",
        "    eot_idx=pos_vocab[sep_token],\n",
        "    t_cal=T_CAL,\n",
        "    transformer=bert,\n",
        ")\n",
        "\n",
        "# crf.to(\"mps\")\n",
        "\n",
        "train_model_report_accuracy(\n",
        "    crf,\n",
        "    LR,\n",
        "    EPOCHS,\n",
        "    train_dataloader,\n",
        "    valid_dataloader,\n",
        "    pad_token_idx,\n",
        "    pos_vocab[pad_token],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j3Xr0HBxhq8"
      },
      "source": [
        "## Q3g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "NgAws_iGxiX6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "naive:  tensor([-1.3186], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1.3186302781105042\n",
            "naive:  tensor([-49.0946], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -49.094612539892346\n",
            "naive:  tensor([-1714.0381], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1714.038061178926\n",
            "naive:  tensor([-2.5707], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2.570695124566555\n",
            "naive:  tensor([-112.1707], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -112.170696859965\n",
            "naive:  tensor([-3851.3411], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -3851.3411815494824\n",
            "naive:  tensor([-0.7064], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -0.7063582446426153\n",
            "naive:  tensor([-36.5544], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -36.554445535377184\n",
            "naive:  tensor([-1574.8135], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1574.8133558685704\n",
            "naive:  tensor([-3.3749], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -3.37493559718132\n",
            "naive:  tensor([-142.5776], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -142.57757297096828\n",
            "naive:  tensor([-4469.0109], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -4469.011107146446\n",
            "naive:  tensor([-2.9450], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2.945048069115728\n",
            "naive:  tensor([-116.8442], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -116.8441945813342\n",
            "naive:  tensor([-4143.4560], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -4143.455977014313\n",
            "naive:  tensor([-1.8578], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1.8577574834343977\n",
            "naive:  tensor([-86.1656], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -86.16563494848302\n",
            "naive:  tensor([-2577.8663], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2577.866286972102\n",
            "naive:  tensor([-1.3296], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1.3295524716377258\n",
            "naive:  tensor([-41.1464], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -41.146388451686676\n",
            "naive:  tensor([-1825.6179], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1825.6178149732245\n",
            "naive:  tensor([-1.9985], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1.9985144920647144\n",
            "naive:  tensor([-75.2316], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -75.23159066423017\n",
            "naive:  tensor([-2513.3023], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2513.302419951422\n",
            "naive:  tensor([-1.7907], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1.790698376717046\n",
            "naive:  tensor([-86.7706], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -86.7705734346383\n",
            "naive:  tensor([-3281.0676], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -3281.0676523613897\n",
            "naive:  tensor([-2.2732], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2.2732399727683514\n",
            "naive:  tensor([-90.6482], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -90.64821945993492\n",
            "naive:  tensor([-2681.5251], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2681.5250621497507\n",
            "naive:  tensor([-2.2669], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2.2669304981827736\n",
            "naive:  tensor([-88.2637], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -88.2636931100682\n",
            "naive:  tensor([-2449.5634], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2449.5633444922823\n",
            "naive:  tensor([-1.7465], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1.7464760625734925\n",
            "naive:  tensor([-76.8334], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -76.8334481486448\n",
            "naive:  tensor([-2984.1929], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2984.1929256268286\n",
            "naive:  tensor([-1.5346], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1.5346105736680329\n",
            "naive:  tensor([-39.7476], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -39.747561603439635\n",
            "naive:  tensor([-1317.4613], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1317.4612478328477\n",
            "naive:  tensor([-2.1332], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2.1331789029427455\n",
            "naive:  tensor([-48.5211], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -48.52107500686452\n",
            "naive:  tensor([-1755.3012], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1755.3012289145295\n",
            "naive:  tensor([-1.9441], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1.9441474676132202\n",
            "naive:  tensor([-65.1931], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -65.19305366420507\n",
            "naive:  tensor([-2472.4785], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2472.47843553937\n",
            "naive:  tensor([-1.6703], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1.6703244168311357\n",
            "naive:  tensor([-74.8360], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -74.83600996966734\n",
            "naive:  tensor([-1952.2861], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1952.286246088819\n",
            "naive:  tensor([-2.1843], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2.184274955885485\n",
            "naive:  tensor([-75.8638], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -75.8638352368624\n",
            "naive:  tensor([-2766.3400], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2766.339889634859\n",
            "naive:  tensor([-1.1743], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1.1742630526423454\n",
            "naive:  tensor([-75.1589], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -75.15891437029812\n",
            "naive:  tensor([-2123.0903], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2123.0903888025027\n",
            "naive:  tensor([-1.3557], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1.3556747753173113\n",
            "naive:  tensor([-62.0284], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -62.028425526568135\n",
            "naive:  tensor([-2265.7368], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2265.7367825028277\n",
            "naive:  tensor([-1.3708], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1.3708225600421429\n",
            "naive:  tensor([-46.2213], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -46.221263859102486\n",
            "naive:  tensor([-1210.5977], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1210.597785622525\n",
            "naive:  tensor([-0.6834], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -0.6833840757608414\n",
            "naive:  tensor([-32.8332], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -32.83318964392368\n",
            "naive:  tensor([-1112.4739], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1112.4737556233144\n",
            "naive:  tensor([-1.3494], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1.3493918422609568\n",
            "naive:  tensor([-60.0682], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -60.06817128304061\n",
            "naive:  tensor([-2194.0033], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2194.00339218671\n",
            "naive:  tensor([-2.4767], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2.476710641756654\n",
            "naive:  tensor([-121.7900], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -121.78995858012594\n",
            "naive:  tensor([-4177.7699], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -4177.76993109208\n",
            "naive:  tensor([-1.6338], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1.6337843148503453\n",
            "naive:  tensor([-83.6978], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -83.69776641712382\n",
            "naive:  tensor([-2474.7125], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2474.7125426859525\n",
            "naive:  tensor([-2.1012], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2.1011886345222592\n",
            "naive:  tensor([-72.2054], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -72.20537263164644\n",
            "naive:  tensor([-2484.5709], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2484.57078369256\n",
            "naive:  tensor([-0.6749], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -0.6749315746128559\n",
            "naive:  tensor([-28.1509], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -28.150951668721813\n",
            "naive:  tensor([-1053.6122], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1053.6122453325274\n",
            "naive:  tensor([-2.4988], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2.4988153092563152\n",
            "naive:  tensor([-107.9471], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -107.94713378068624\n",
            "naive:  tensor([-3337.6433], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -3337.6433618244914\n",
            "naive:  tensor([-1.4937], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1.4936997160548344\n",
            "naive:  tensor([-69.9636], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -69.96361727773755\n",
            "naive:  tensor([-2286.4057], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2286.405543654164\n",
            "naive:  tensor([-2.4531], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2.453095921315253\n",
            "naive:  tensor([-77.0718], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -77.07180141717015\n",
            "naive:  tensor([-1984.8897], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1984.8895453234263\n",
            "naive:  tensor([-2.2820], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -2.282028515357524\n",
            "naive:  tensor([-97.8889], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -97.88887604660003\n",
            "naive:  tensor([-3004.5698], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -3004.5698776850177\n",
            "naive:  tensor([-1.4288], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1.4288460256066173\n",
            "naive:  tensor([-48.5587], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -48.55868108596107\n",
            "naive:  tensor([-1670.9119], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -1670.911991806679\n",
            "naive:  tensor([-3.3202], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -3.320204879157245\n",
            "naive:  tensor([-151.7873], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -151.7873452624033\n",
            "naive:  tensor([-4997.5170], dtype=torch.float64, grad_fn=<SubBackward0>)\n",
            "own:    -4997.517051475225\n"
          ]
        }
      ],
      "source": [
        "emissions = crf.calculate_emissions(W_train)\n",
        "\n",
        "stc = 21 \n",
        "widx = 4\n",
        "\n",
        "for sentence in range(W_train.shape[0]):\n",
        "    for word_index in [2, 3, 4]:\n",
        "        print(f\"naive:  {crf.entropy_naive(W_train[sentence, :word_index], emissions[sentence])}\")\n",
        "        print(f\"own:    {crf.backward_entropy(W_train[:, 1:word_index], emissions)[sentence]}\")\n",
        "        assert torch.isclose(\n",
        "            crf.entropy_naive(\n",
        "                W_train[sentence, :word_index],\n",
        "                emissions[\n",
        "                    sentence,\n",
        "                ],\n",
        "            ),\n",
        "            crf.backward_entropy(W_train[:, 1:word_index], emissions)[sentence],\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NgAws_iGxiX6"
      },
      "outputs": [],
      "source": [
        "emissions = crf.calculate_emissions(W_train)\n",
        "\n",
        "for sentence in range(W_train.shape[0]):\n",
        "    for word_index in [2, 3, 4]:\n",
        "        assert torch.isclose(\n",
        "            crf.entropy_naive(\n",
        "                W_train[sentence, :word_index],\n",
        "                emissions[\n",
        "                    sentence,\n",
        "                ],\n",
        "            ),\n",
        "            crf.backward_entropy(W_train[:, 1:word_index], emissions)[sentence],\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "jIKnm3b_Bm1N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, batch 0\n",
            "Epoch 0, batch 20\n",
            "Epoch 0, batch 40\n",
            "Epoch 0, batch 60\n",
            "Epoch 0, batch 80\n",
            "Epoch 0, batch 100\n",
            "Epoch 0, batch 120\n",
            "Epoch 0, batch 140\n",
            "Epoch 0, batch 160\n",
            "Epoch 0, batch 180\n",
            "Epoch 0, batch 200\n",
            "Epoch 0, batch 220\n",
            "Epoch 0, batch 240\n",
            "Epoch 0, batch 260\n",
            "Epoch 0, batch 280\n",
            "Epoch 0, batch 300\n",
            "Epoch 0, batch 320\n",
            "Epoch 0, batch 340\n",
            "Epoch 0, batch 360\n",
            "Epoch 0, batch 380\n",
            "-------------------------\n",
            "Epoch: 1 / 3\n",
            "Development set accuracy: 0.9010432362556458\n",
            "-------------------------\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'pickle' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [35], line 16\u001b[0m\n\u001b[1;32m      3\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(SEED)\n\u001b[1;32m      5\u001b[0m entropy_regularized_crf \u001b[39m=\u001b[39m NeuralCRF(\n\u001b[1;32m      6\u001b[0m     pad_idx_word\u001b[39m=\u001b[39mpad_token_idx,\n\u001b[1;32m      7\u001b[0m     pad_idx_pos\u001b[39m=\u001b[39mpos_vocab[pad_token],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     beta\u001b[39m=\u001b[39m\u001b[39m10.0\u001b[39m,\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m train_model_report_accuracy(\n\u001b[1;32m     17\u001b[0m     entropy_regularized_crf,\n\u001b[1;32m     18\u001b[0m     LR,\n\u001b[1;32m     19\u001b[0m     EPOCHS,\n\u001b[1;32m     20\u001b[0m     train_dataloader,\n\u001b[1;32m     21\u001b[0m     valid_dataloader,\n\u001b[1;32m     22\u001b[0m     pad_token_idx,\n\u001b[1;32m     23\u001b[0m     pos_vocab[pad_token],\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m pickle\u001b[39m.\u001b[39mdump(crf, \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mcrf_b10\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n",
            "Cell \u001b[0;32mIn [34], line 71\u001b[0m, in \u001b[0;36mtrain_model_report_accuracy\u001b[0;34m(crf, lr, epochs, train_dataloader, dev_dataloader, pad_token_idx_word, pad_token_idx_tag)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDevelopment set accuracy: \u001b[39m\u001b[39m{\u001b[39;00macc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m         pickle\u001b[39m.\u001b[39mdump(crf, \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcrf_epoch\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     72\u001b[0m     epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "entropy_regularized_crf = NeuralCRF(\n",
        "    pad_idx_word=pad_token_idx,\n",
        "    pad_idx_pos=pos_vocab[pad_token],\n",
        "    bos_idx=init_token_idx,\n",
        "    eos_idx=sep_token_idx,\n",
        "    bot_idx=pos_vocab[init_token],\n",
        "    eot_idx=pos_vocab[sep_token],\n",
        "    t_cal=T_CAL,\n",
        "    transformer=bert,\n",
        "    beta=10.0,\n",
        ")\n",
        "train_model_report_accuracy(\n",
        "    entropy_regularized_crf,\n",
        "    LR,\n",
        "    EPOCHS,\n",
        "    train_dataloader,\n",
        "    valid_dataloader,\n",
        "    pad_token_idx,\n",
        "    pos_vocab[pad_token],\n",
        ")\n",
        "\n",
        "pickle.dump(crf, open(\"crf_b10\", 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "wdhhpDDfyj-8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, batch 0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [45], line 20\u001b[0m\n\u001b[1;32m      7\u001b[0m pickle\u001b[39m.\u001b[39mdump(crf, \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mcrf_b1_0\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      9\u001b[0m entropy_regularized_crf \u001b[39m=\u001b[39m NeuralCRF(\n\u001b[1;32m     10\u001b[0m     pad_idx_word\u001b[39m=\u001b[39mpad_token_idx,\n\u001b[1;32m     11\u001b[0m     pad_idx_pos\u001b[39m=\u001b[39mpos_vocab[pad_token],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     beta\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m,\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 20\u001b[0m train_model_report_accuracy(\n\u001b[1;32m     21\u001b[0m     entropy_regularized_crf,\n\u001b[1;32m     22\u001b[0m     LR,\n\u001b[1;32m     23\u001b[0m     EPOCHS,\n\u001b[1;32m     24\u001b[0m     train_dataloader,\n\u001b[1;32m     25\u001b[0m     valid_dataloader,\n\u001b[1;32m     26\u001b[0m     pad_token_idx,\n\u001b[1;32m     27\u001b[0m     pos_vocab[pad_token],\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m pickle\u001b[39m.\u001b[39mdump(crf, \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mcrf_b1_0\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n",
            "Cell \u001b[0;32mIn [43], line 41\u001b[0m, in \u001b[0;36mtrain_model_report_accuracy\u001b[0;34m(crf, lr, epochs, train_dataloader, dev_dataloader, pad_token_idx_word, pad_token_idx_tag)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m crf\u001b[39m.\u001b[39mparameters():\n\u001b[1;32m     40\u001b[0m     param\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m loss \u001b[39m=\u001b[39m crf\u001b[39m.\u001b[39;49mloss(T, W)\n\u001b[1;32m     42\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     43\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
            "Cell \u001b[0;32mIn [41], line 187\u001b[0m, in \u001b[0;36mNeuralCRF.loss\u001b[0;34m(self, T, W)\u001b[0m\n\u001b[1;32m    185\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnegative(torch\u001b[39m.\u001b[39mmean(scores \u001b[39m-\u001b[39m log_normalizer))\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m--> 187\u001b[0m     unnormalized_entropy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackward_entropy(\n\u001b[1;32m    188\u001b[0m         W[:, \u001b[39m1\u001b[39;49m:], emissions\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    190\u001b[0m     entropy \u001b[39m=\u001b[39m (\n\u001b[1;32m    191\u001b[0m         (unnormalized_entropy \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39mexp(log_normalizer))\n\u001b[1;32m    192\u001b[0m         \u001b[39m+\u001b[39m log_normalizer\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    194\u001b[0m     \u001b[39mreturn\u001b[39;00m loss \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mnegative(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mmean(entropy))\n",
            "Cell \u001b[0;32mIn [41], line 625\u001b[0m, in \u001b[0;36mNeuralCRF.backward_entropy\u001b[0;34m(self, W, emissions)\u001b[0m\n\u001b[1;32m    623\u001b[0m         w \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(emissions[:, i, :] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransitions[\u001b[39mNone\u001b[39;00m, t1, :])\n\u001b[1;32m    624\u001b[0m         y \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mw \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mlog(w)\n\u001b[0;32m--> 625\u001b[0m         beta[:, i, t1, \u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(w \u001b[39m*\u001b[39m beta[:, i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, :, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mclone(), \u001b[39m1\u001b[39m)\n\u001b[1;32m    626\u001b[0m         beta[:, i, t1, \u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(w \u001b[39m*\u001b[39m beta[:, i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, :, \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mclone() \u001b[39m+\u001b[39m y \u001b[39m*\u001b[39m beta[:, i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, :, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mclone(), \u001b[39m1\u001b[39m)\n\u001b[1;32m    628\u001b[0m     \u001b[39m# with open(\"beta.txt\", \"a\") as f:\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39m#     f.write(f\"beta[{i}] {beta.shape}=\\n{beta[0, i, :, 0]}, {beta[0, i, :, 1]}\\n\")\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \n\u001b[1;32m    631\u001b[0m \u001b[39m# print(beta[0, :, :, :])\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/traceback.py:213\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     f \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39m_getframe()\u001b[39m.\u001b[39mf_back\n\u001b[0;32m--> 213\u001b[0m \u001b[39mreturn\u001b[39;00m format_list(extract_stack(f, limit\u001b[39m=\u001b[39;49mlimit))\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/traceback.py:227\u001b[0m, in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     f \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39m_getframe()\u001b[39m.\u001b[39mf_back\n\u001b[0;32m--> 227\u001b[0m stack \u001b[39m=\u001b[39m StackSummary\u001b[39m.\u001b[39;49mextract(walk_stack(f), limit\u001b[39m=\u001b[39;49mlimit)\n\u001b[1;32m    228\u001b[0m stack\u001b[39m.\u001b[39mreverse()\n\u001b[1;32m    229\u001b[0m \u001b[39mreturn\u001b[39;00m stack\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/traceback.py:364\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    362\u001b[0m result \u001b[39m=\u001b[39m klass()\n\u001b[1;32m    363\u001b[0m fnames \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m--> 364\u001b[0m \u001b[39mfor\u001b[39;00m f, lineno \u001b[39min\u001b[39;00m frame_gen:\n\u001b[1;32m    365\u001b[0m     co \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mf_code\n\u001b[1;32m    366\u001b[0m     filename \u001b[39m=\u001b[39m co\u001b[39m.\u001b[39mco_filename\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/traceback.py:317\u001b[0m, in \u001b[0;36mwalk_stack\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     f \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39m_getframe()\u001b[39m.\u001b[39mf_back\u001b[39m.\u001b[39mf_back\n\u001b[0;32m--> 317\u001b[0m \u001b[39mwhile\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[39myield\u001b[39;00m f, f\u001b[39m.\u001b[39mf_lineno\n\u001b[1;32m    319\u001b[0m     f \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mf_back\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "entropy_regularized_crf = NeuralCRF(\n",
        "    pad_idx_word=pad_token_idx,\n",
        "    pad_idx_pos=pos_vocab[pad_token],\n",
        "    bos_idx=init_token_idx,\n",
        "    eos_idx=sep_token_idx,\n",
        "    bot_idx=pos_vocab[init_token],\n",
        "    eot_idx=pos_vocab[sep_token],\n",
        "    t_cal=T_CAL,\n",
        "    transformer=bert,\n",
        "    beta=1.0,\n",
        ")\n",
        "train_model_report_accuracy(\n",
        "    entropy_regularized_crf,\n",
        "    LR,\n",
        "    EPOCHS,\n",
        "    train_dataloader,\n",
        "    valid_dataloader,\n",
        "    pad_token_idx,\n",
        "    pos_vocab[pad_token],\n",
        ")\n",
        "\n",
        "pickle.dump(crf, open(\"crf_b1_0\", 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmCPsmTtym3U"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/combining.py:248: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, batch 0\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'NeuralCRF' object has no attribute 'device'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [74], line 16\u001b[0m\n\u001b[1;32m      3\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(SEED)\n\u001b[1;32m      5\u001b[0m entropy_regularized_crf \u001b[39m=\u001b[39m NeuralCRF(\n\u001b[1;32m      6\u001b[0m     pad_idx_word\u001b[39m=\u001b[39mpad_token_idx,\n\u001b[1;32m      7\u001b[0m     pad_idx_pos\u001b[39m=\u001b[39mpos_vocab[pad_token],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     beta\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m train_model_report_accuracy(\n\u001b[1;32m     17\u001b[0m     entropy_regularized_crf,\n\u001b[1;32m     18\u001b[0m     LR,\n\u001b[1;32m     19\u001b[0m     EPOCHS,\n\u001b[1;32m     20\u001b[0m     train_dataloader,\n\u001b[1;32m     21\u001b[0m     valid_dataloader,\n\u001b[1;32m     22\u001b[0m     pad_token_idx,\n\u001b[1;32m     23\u001b[0m     pos_vocab[pad_token],\n\u001b[1;32m     24\u001b[0m )\n",
            "Cell \u001b[0;32mIn [47], line 40\u001b[0m, in \u001b[0;36mtrain_model_report_accuracy\u001b[0;34m(crf, lr, epochs, train_dataloader, dev_dataloader, pad_token_idx_word, pad_token_idx_tag)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m crf\u001b[39m.\u001b[39mparameters():\n\u001b[1;32m     39\u001b[0m     param\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m loss \u001b[39m=\u001b[39m crf\u001b[39m.\u001b[39;49mloss(T, W)\n\u001b[1;32m     41\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     42\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
            "Cell \u001b[0;32mIn [71], line 180\u001b[0m, in \u001b[0;36mNeuralCRF.loss\u001b[0;34m(self, T, W)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39m\"\"\"Calculate the loss for a batch.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39m    Mean loss for the batch.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    179\u001b[0m emissions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculate_emissions(W)\n\u001b[0;32m--> 180\u001b[0m emissions\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    181\u001b[0m \u001b[39m# Note that we have to handle paddings and EOS within the score\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39m# and backward functions, but we can already skip the BOS tokens\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m# here.\u001b[39;00m\n\u001b[1;32m    184\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscore(emissions, W[:, \u001b[39m1\u001b[39m:], T[:, \u001b[39m1\u001b[39m:])\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NeuralCRF' object has no attribute 'device'"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "entropy_regularized_crf = NeuralCRF(\n",
        "    pad_idx_word=pad_token_idx,\n",
        "    pad_idx_pos=pos_vocab[pad_token],\n",
        "    bos_idx=init_token_idx,\n",
        "    eos_idx=sep_token_idx,\n",
        "    bot_idx=pos_vocab[init_token],\n",
        "    eot_idx=pos_vocab[sep_token],\n",
        "    t_cal=T_CAL,\n",
        "    transformer=bert,\n",
        "    beta=0.1,\n",
        ")\n",
        "train_model_report_accuracy(\n",
        "    entropy_regularized_crf,\n",
        "    LR,\n",
        "    EPOCHS,\n",
        "    train_dataloader,\n",
        "    valid_dataloader,\n",
        "    pad_token_idx,\n",
        "    pos_vocab[pad_token],\n",
        ")\n",
        "\n",
        "pickle.dump(crf, open(\"crf_b0_1\", 'wb'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "bl-a_j9oemGI",
        "BdHg2VJUt5VY",
        "BoqeybSRvDyC",
        "zz5Gely7wU9X",
        "7j3Xr0HBxhq8"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 ('nlp')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8 (main, Nov 24 2022, 08:08:27) [Clang 14.0.6 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "4e1bd50c29ca9d85d25a0084185febf7020f5e8bd5da929d18cf95b16060538f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
