\documentclass[a4paper,12pt]{ETHexercise}
\usepackage{bbm}

\input{preamble}

\usepackage{multirow}
\title{NLP Assignment}
\begin{document}
\setserie{1}


\newcommand{\pair}[2]{{\langle #1 , #2 \rangle}}
\newcommand{\score}[2]{\text{score}_{\theta}(\langle #1, #2 \rangle, \boldsymbol{w})}
\newcommand{\sscore}[1]{\text{score}_{\theta}(#1, \boldsymbol{w})}
\newcommand*{\eos}{\text{EOS}}

\lectureheader{Prof. Ryan Cotterell}
{}
{\Large Natural Language Processing}{Fall 2022}
\begin{center}
	{\Huge Simon Wachter: Assignment 4}\\
	\quad\newline
	siwachte@ethz.ch, 19-920-198\\
	\quad\newline
	\timestamp
\end{center}
\begin{question}\\
	\begin{subquestion}
		\begin{align}
			\sum_{w \in \Sigma^*} \tilde{p}(w) & = \sum_{w \in \Sigma^*, |w|=1} \tilde{p}(w) + \sum_{w \in \Sigma^*, |w|=2} \tilde{p}(w) + \sum_{w \in \Sigma^*, |w|=3} \tilde{p}(w) + \dots \\
			                                   & = \sum_{w \in \Sigma} \tilde{p}(w) + \sum_{w \in \Sigma^*, |w|=2} \tilde{p}(w) + \sum_{w \in \Sigma^*, |w|=3} \tilde{p}(w) + \dots          \\
			                                   & = 1 + \sum_{w \in \Sigma^*, |w|=2} \tilde{p}(w) + \sum_{w \in \Sigma^*, |w|=3} \tilde{p}(w) + \dots                                         \\
			                                   & = 1 + \sum_{w \in \Sigma, |w|=1} \sum_{w' \in \Sigma^*} \tilde{p}(w) \tilde{p}(w') + \sum_{w \in \Sigma^*, |w|=3} \tilde{p}(w) + \dots      \\
			                                   & = 1 + \sum_{w \in \Sigma} \tilde{p}(w) \sum_{w' \in \Sigma^*, |w|=1} \tilde{p}(w') + \sum_{w \in \Sigma^*, |w|=3} \tilde{p}(w) + \dots      \\
			                                   & = 1 + \sum_{w \in \Sigma} \tilde{p}(w) \sum_{w' \in \Sigma} \tilde{p}(w') + \sum_{w \in \Sigma^*, |w|=3} \tilde{p}(w) + \dots               \\
			                                   & = 1 + 1 \cdot 1 + \sum_{w \in \Sigma^*, |w|=3} \tilde{p}(w) + \dots                                                                         \\
			                                   & = 1 + 1 \cdot 1 + \sum_{w \in \Sigma}  \sum_{w \in \Sigma^*, |w|=2} \tilde{p}(w) \tilde{p}(w') + \dots                                      \\
			                                   & = 1 + 1 \cdot 1 + \sum_{w \in \Sigma} \tilde{p}(w) \sum_{w \in \Sigma^*, |w|=2} \tilde{p}(w') + \dots                                       \\
			                                   & = 1 + 1 \cdot 1 + 1 \cdot 1 \cdot 1 + \dots                                                                                                 \\
			                                   & = \sum_{i=1}^{\infty} 1^{i}                                                                                         \label{1a_inf_series}
		\end{align}
		We can see that the series in \eqref{1a_inf_series} diverges to $\infty$.
	\end{subquestion}
	\begin{subquestion}
		We first state some equations that are used later:
		\begin{align}
			\sum_{w \in \Sigma \cup \{\eos\}} p(w) & = 1 \label{1b_sum_p}                                                                                 \\
			\Rightarrow \sum_{w \in \Sigma} p(w)   & = 1 - p(\eos)                                                                                        \\
			\sum_{w \in \Sigma} p(w)               & < 1                                                                                                  \\
			\sum_{w \in \Sigma} p(\eos)p(w)        & = p(\eos) \sum_{w \in \Sigma} p(w)                                                                   \\
			\sum_{w \in \Sigma^*, |w|=0} p(w)      & = p(\eos)                          &  & \text{because we have only one EOS symbol} \label{1b_sum_p0} \\
		\end{align}
		We define $l$ as:
		\begin{align}
			l & = \sum_{w \in \Sigma} p(w) \label{1b_def_l} \\
			  & = (1 - p(\eos)) \label{1b_def_l2}           \\
			l & < 1
		\end{align}
		Now we can formulate the given equation:
		\begin{align}
			\sum_{w \in \Sigma^*} p(w) & =  \sum_{w \in \Sigma^*, |w|=0} p(w) + \sum_{w \in \Sigma^*, |w|=1} p(w) + \sum_{w \in \Sigma^*, |w|=2} p(w) + \sum_{w \in \Sigma^*, |w|=3} p(w) + \dots                                                       \\
			                           & = p(\eos) + \sum_{w \in \Sigma^*, |w|=1} p(w) + \sum_{w \in \Sigma^*, |w|=2} p (w) + \sum_{w \in \Sigma^*, |w|=3} p(w) + \dots                                                                                 \\
			                           & = p(\eos) \left( 1 + \sum_{w \in \Sigma^*, |w|=1} \tilde{p}(w) + \sum_{w \in \Sigma^*, |w|=2} \tilde{p}(w) + \sum_{w \in \Sigma^*, |w|=3} \tilde{p}(w) + \dots                                         \right) \\
			                           & = p(\eos) \left( 1 + \sum_{w \in \Sigma^*, |w|=1} \tilde{p}(w) + \sum_{w \in \Sigma^*, |w|=2}  \tilde{p}(w) + \sum_{w \in \Sigma^*, |w|=3}  \tilde{p}(w) + \dots  \right)                                      \\
			                           & =  p(\eos) \left( 1 + \sum_{w \in \Sigma} p(w) + \sum_{w \in \Sigma^*, |w|=2}  \tilde{p}(w) + \sum_{w \in \Sigma^*, |w|=3}  \tilde{p}(w) + \dots \right)                                                       \\
			                           & = p(\eos) \left( 1 + l + \sum_{w \in \Sigma^*, |w|=2}  \tilde{p}(w) + \sum_{w \in \Sigma^*, |w|=3}  \tilde{p}(w) + \dots  \right)                                                                              \\
			                           & = p(\eos) \left( 1 + l + \sum_{w \in \Sigma} \sum_{w' \in \Sigma} p(w)p(w') + \sum_{w \in \Sigma^*, |w|=3}  \tilde{p}(w) + \dots \right)                                                                       \\
			                           & = p(\eos) \left( 1 + l +  \sum_{w \in \Sigma} p(w) \sum_{w' \in \Sigma} p(w') + \sum_{w \in \Sigma^*, |w|=3}  \tilde{p}(w) + \dots  \right)                                                                    \\
			                           & = p(\eos) \left( 1 + l + l^2 + \sum_{w \in \Sigma^*, |w|=3}  \tilde{p}(w) + \dots   \right)                                                                                                                    \\
			                           & = p(\eos) \left( 1 + l + l^2 + l^3 + \dots     \right)                                                                                                                                                         \\
			                           & = p(\eos) \left(\sum_{n=0}^{\infty} l^n                                                 \right)                                                                                                                \\
			                           & = p(\eos) \frac{1}{1-l} \quad\quad \text{(limit geom. series, because $l < 1$)} \label{1b_sum_p2}                                                                                                              \\
			                           & = \frac{p(\eos)}{1 - (1-p(\eos))}                                                                                                                                                                              \\
			                           & = \frac{p(\eos)}{p(\eos)}                                                                                                                                                                                      \\
			                           & = 1
		\end{align}
	\end{subquestion}
	\begin{subquestion}
		\begin{align}
			\sum_{u \in \Sigma^*} p(wu) & = \sum_{u \in \Sigma^*} p(\eos | wu) p_{pre}(u|w)p_{pre}(w)                &  & \text{by def.}    \\
			                            & = p_{pre}(w) \left( \sum_{u \in \Sigma^*} p(\eos | wu) p_{pre}(u|w)\right)                        \\
			                            & = p_{pre}(w) \left( \sum_{u \in \Sigma^*}p(u | w) \right)                  &  & \text{def. p(w)}  \\
			                            & = p_{pre}(w) \left( \sum_{u \in \Sigma^*} \frac{p(w|u)p(u)}{p(w)} \right)  &  & \text{Bayes rule} \\
			                            & = p_{pre}(w) \left( \frac{p(w)}{p(w)}\right)                               &  & \text{Bayes rule} \\
			                            & = p_{pre}(w)
		\end{align}
	\end{subquestion}
	\begin{subquestion}
		We use CKY with the $(+, \times)$-semiring. Further we use $\log p$ insted of $p$ for our score function. These operations will lead to $p(w_1 \dots w_n | S)$ being calculated at the top of the tree because we fill the tree according to:
		\begin{align}
			\text{s} [i,k,X] & = \sum_{k}^{} \sum_{i} \sum_{j} \exp (\text{score} (X \rightarrow YZ)) \times \text{s} [i,j,Y] \times \text{s} [j,k,Z] \\
			                 & = \exp(\log(p(YZ | X))) \times p(w_i \dots w_j | Y) \times p(w_{j+1} \dots w_k | Z)                                    \\
			                 & = p(YZ | X) \times p(w_i \dots w_j | Y) \times p(w_{j+1} \dots w_k | Z)                                                \\
			                 & = p(w_i \dots w_k | X)
		\end{align}
		We can the easily multiply $p(w_1 \dots w_n | S)$ by $p(\text{EOS})$ and get the desired result.
	\end{subquestion}
	\begin{subquestion}
		\begin{align}
			\sum_{u \in \Sigma^*} p(wu) & = \sum_{u \in \Sigma^*} p_{\text{inside}}(wu | S)     \\
			                            & = p(X \overset{*}{\Rightarrow} w_i \dots w_k)         \\
			                            & = p(S \overset{*}{\Rightarrow} wv) \label{eq:1e_conc} \\
		\end{align}
		\cref*{eq:1e_conc} holds, because summing over all possible suffixes is equivalent to allow all different derivation trees that produce $v$.
	\end{subquestion}
	\begin{subquestion}
		Following the hint given in the exercise, we create a matrix $M \in |\mathcal{N}| \times |\mathcal{N}|$ where each entry $M_{X,Y}$ corresponds to the probability of deriving $Y$ from $X$:
		\begin{align}
			M_{Y,X} & = p(X \rightarrow Y\alpha)
		\end{align}
		We can see that by multiplying in the inside semiring this matrix with itself we get the probability of deriving $i$ from $j$ in two steps and so on. Thus, the kleene star over $M$ will yield the desired property. To derive the kleene star we use Lehmann's algorithm:
		\begin{align}
			M^* & =\bigotimes_{k=0}^{\infty} M      \\
			    & = I + M \bigotimes_{k=0}^{\infty} \\
			    & = I + MM^*
		\end{align}
		\begin{align}
			M^*  - MM^* & =I                                 \\
			(I- M)M^*   & = I                                \\
			M^*         & = (I - M)^{-1} \label{eq:m_kleene}
		\end{align}
		As stated in the lecture notes, for \cref*{eq:m_kleene} to holde we require the largest eigenvalue of $M$ to be smaller than $1$. This translates to the condition that all diagonal entries of $M$ have to be $< 1$. The diagonal entries correspond to the probability of deriving a symbol from itself. This probability has to be smaller than $1$ in our case, since otherwise we would not be able to generate any other symbol from it than itself, which would results in infinite sentences.\\
		Then, using \cref{eq:m_kleene} we can calculate $M^*$ in $\mathcal{O}(|\mathcal{N}|^3)$. The entries in the matrix now correspond to:
		\begin{align}
			M^*_{X,Y} = p[X \overset{*}{\Rightarrow} Y\alpha] & = p_{\text{lc}}(Y | X)
		\end{align}
		From this matrix we further need to derive the $p_{\text{lc}}(YZ | X)$. We iterate over all possible choices for $X, X',Y,Z$ to derive:
		\begin{align}
			p_{\text{lc}}(YZ | X) & = \sum_{X' \in \Sigma} p_{\text{lc}}(X' | X) p_{\text{lc}}(Y | X') p_{\text{lc}}(Z | X'Y) \label{eq:lc}
		\end{align}
		Which can be done in $\mathcal{O}(|\mathcal{N}|^4)$.
	\end{subquestion}
	\begin{subquestion}
		\begin{align}
			p_{pre}(w_i \dots w_k | X)                                    & = \sum_{j=1}^{k-1} \sum_{Y,Z \in \mathcal{N}} {\color{red} p(X \overset{*}{\Rightarrow} YZ\alpha)}{\color{green}p(Y \overset{*}{\Rightarrow} w_i \dots w_j)}{\color{blue} p(Z \overset{*}{\Rightarrow} w_{j+1} \dots w_k)} \label{eq:pre} \\
			{\color{red}p(X \overset{*}{\Rightarrow} YZ\alpha)}           & = \text{probability of deriving the two subtrees $Y$ and $Z$ from $X$} \label{eq:pre2}                                                                                                                                                    \\
			{\color{green}p(Y \overset{*}{\Rightarrow} w_i \dots w_j)}    & = \text{probability of deriving the string $w_i \dots w_j$ from $Y$} \label{eq:pre3}                                                                                                                                                      \\
			{\color{blue}p(Z \overset{*}{\Rightarrow} w_{j+1} \dots w_k)} & = \text{probability of deriving the string $w_{j+1} \dots w_k$ from $Z$} \label{eq:pre4}                                                                                                                                                  \\
			                                                              & \sum_{j=1}^{k-1} \sum_{Y,Z \in \mathcal{N}} {\color{red} p(X \overset{*}{\Rightarrow} YZ\alpha)}{\color{green}p(Y \overset{*}{\Rightarrow} w_i \dots w_j)}{\color{blue} p(Z \overset{*}{\Rightarrow} w_{j+1} \dots w_k)}                  \\& =\sum_{j=1}^{k-1} \sum_{Y,Z \in \mathcal{N}}p_{\text{lc}}(YZ | X) p_{\text{inside}}(w_i \dots w_j | Y) p_{\text{pre}}(w_{j+1} \dots w_k | Z) \label{eq:pre5}                                                                               \\
		\end{align}
	\end{subquestion}
	\begin{subquestion}
		Clarify:
		We can see that if we have all precomputation done, then the calculation of $p_{pre}(w')$ can be done in $\mathcal{O}(1)$ for a given subsequence $w'$ of $w$. As $|w| = N$, this has runtime $\mathcal{O}(N)$.
		Now for the precompution, we first look at $p_{\text{lc}}(YZ | X)$. As shown in exercise f) this can be done in $\mathcal{O}(|\mathcal{N}|^4)$.
		Equation (15) on the exercise sheet shows that $p_{pre}(w_i \dots w_k | X)$ depends only on $p_{\text{lc}}(YZ | X)$, $p_{inside}(w_i \dots w_j)$ and $p_{\text{pre}(w_{j+1} \dots w_k)}$ for $j < k$. The first of the three we have already calculated. We calculate $p_{\text{inside}}$ with the CKY algorithm and save all intermediate values. The size of our grammar is $\mathcal{O}(|\mathcal{N}|^2)$. Thus, CKY runs in $\mathcal{O}(N^3 |\mathcal{N}|^2)$ and we need to run it $\mathcal{O}(|\mathcal{N}|)$ times for each $X \in \Sigma$.
		\newline
		After these steps we have all our value precomputed in $\mathcal{O}(N^3 |\mathcal{N}|^3 + |\mathcal{N}|^4)$.
		We then invoke the algorithm naively for each subsequence of $w_1 \dots w_i$. This gives us a runtime of $\mathcal{O}(N^4 |\mathcal{N}|^3 + N|\mathcal{N}|^4)$.
	\end{subquestion}
	\begin{align}
		s
	\end{align}
\end{question}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End: